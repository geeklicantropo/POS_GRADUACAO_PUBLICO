{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lwF0mz5zwoy"
      },
      "source": [
        "# **Coletando Dados**\n",
        "\n",
        "Os dados foram disponibilizados em meu github público. Eu subi esses arquivos no github e estou usando esse script pronto para carregar esses arquivos no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9h6J6_azmWV",
        "outputId": "f22b9a5d-7cf7-428c-ab47-1f8e07df52a4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "# URL do arquivo .zip no GitHub\n",
        "url = \"https://github.com/geeklicantropo/MESTRADO_PUBLICO/raw/main/DATA_MINING/TAREFAS/Tarefa2/store-sales-time-series-forecasting.zip\"\n",
        "\n",
        "# Pasta onde os arquivos serão extraídos\n",
        "extract_to = \"/content/\"\n",
        "\n",
        "# Baixar o arquivo .zip\n",
        "response = requests.get(url)\n",
        "zip_file_path = os.path.join(extract_to, \"store-sales-time-series-forecasting.zip\")\n",
        "\n",
        "# Verificar se a resposta foi bem sucedida\n",
        "if response.status_code == 200:\n",
        "    # Salvar o arquivo .zip no disco\n",
        "    with open(zip_file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Verificar se o arquivo salvo é um zip válido\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(\"Arquivos extraídos com sucesso!\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"Erro: O arquivo baixado não é um arquivo ZIP válido.\")\n",
        "\n",
        "    # Remover o arquivo .zip baixado\n",
        "    os.remove(zip_file_path)\n",
        "else:\n",
        "    print(f\"Erro ao baixar o arquivo: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gCLmq-bI2H7"
      },
      "source": [
        "# **Objetivo: Prever os preços das próximas vendas com a menor margem de erro possível**\n",
        "\n",
        "**Obs.:**\n",
        "\n",
        "* O Equador é um país cuja economia é altamente dependente do petróleo, tornando-se vulnerável a choques nos preços desse recurso.\n",
        "\n",
        "* Os salários do setor público são pagos quinzenalmente, nos dias 15 e 30, o que pode influenciar as vendas nos supermercados.\n",
        "\n",
        "* Um terremoto de magnitude 7,8 atingiu o Equador em 16 de abril de 2016. Esforços de socorro envolveram a doação de água e produtos de primeira necessidade, impactando significativamente as vendas nos supermercados por várias semanas após o evento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0eowd86cVIcn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\dask\\dataframe\\__init__.py:31: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "#Importações de bibliotecas para manipulação de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "from datetime import timedelta\n",
        "\n",
        "#Importações de bibliotecas para visualização de dados\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "\n",
        "#Importações de bibliotecas para processamento e validação de dados\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, cross_val_score\n",
        "\n",
        "#Importações de bibliotecas para modelagem de dados\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "#Importações de bibliotecas para métricas e otimização de modelos\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "import optuna\n",
        "\n",
        "#Importações de bibliotecas para utilidades adicionais e paralelização\n",
        "from tqdm import tqdm, trange\n",
        "import pickle\n",
        "import math\n",
        "import multiprocessing\n",
        "from joblib import Parallel, delayed\n",
        "import traceback\n",
        "\n",
        "#Configuração de exibição do pandas\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Feature Engineering</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LOVtSdhEUeAm"
      },
      "outputs": [],
      "source": [
        "''' \n",
        "Função para salvar um dataFrame em CSV e liberar a memória utilizada\n",
        "Essa função foi criada porque fazer o merge de todos os datasets em \n",
        "memória estava quebrando o Kernel e reiniciando o Jupyter Notebook.\n",
        "'''\n",
        "def save_and_cleanup(df, file_name):\n",
        "    #Cria o diretório pai se não existir\n",
        "    parent_dir = os.path.dirname(file_name)\n",
        "    if not os.path.exists(parent_dir):\n",
        "        os.makedirs(parent_dir)\n",
        "    #Salva o dataFrame em CSV\n",
        "    df.to_csv(file_name, index=False)\n",
        "    #Libera a memória\n",
        "    del df\n",
        "    gc.collect()\n",
        "    print(f\"Saved and cleaned up: {file_name}\")\n",
        "\n",
        "#Função para carregar um CSV e mergear com um dataFrame base\n",
        "def load_and_merge(base_df, file_name, on_columns, how='left'):\n",
        "    # Carrega o CSV\n",
        "    df = pd.read_csv(file_name)\n",
        "    #Converte a coluna 'date' para datetime\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "    #Mergeia o dataFrame base com o CSV carregado\n",
        "    merged_df = base_df.merge(df, on=on_columns, how=how)\n",
        "    #Libera a memória\n",
        "    del base_df, df\n",
        "    gc.collect()\n",
        "    print(f\"Merged with: {file_name}\")\n",
        "    return merged_df\n",
        "\n",
        "#Função para realizar a engenharia de features\n",
        "def feature_engineering(data, transactions_path, stores_path, holidays_events_path, oil_path, temp_dir='temp'):\n",
        "    #Data do terremoto\n",
        "    earthquake_date = pd.to_datetime('2016-04-16')\n",
        "    #Dias de pagamento de salários\n",
        "    wage_payment_days = [15, 30, 31]\n",
        "\n",
        "    print(\"Loading and preprocessing initial data...\")\n",
        "    data = data.copy()\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "    data['onpromotion'] = data['onpromotion'].astype('float32')\n",
        "    data['store_nbr'] = data['store_nbr'].astype('int8')\n",
        "\n",
        "    #Remove a coluna 'id' se existir\n",
        "    if 'id' in data.columns:\n",
        "        data = data.drop(columns=['id'])\n",
        "\n",
        "    save_and_cleanup(data, f\"{temp_dir}/data.csv\")\n",
        "\n",
        "    print(\"Loading and saving transactions...\")\n",
        "    #Carrega o CSV de transações\n",
        "    transactions = pd.read_csv(transactions_path)\n",
        "    transactions['date'] = pd.to_datetime(transactions['date'])\n",
        "    transactions['store_nbr'] = transactions['store_nbr'].astype('int8')\n",
        "    #Cria a coluna 'month_day' com o formato mês-dia\n",
        "    transactions['month_day'] = transactions['date'].dt.strftime('%m-%d')\n",
        "    #Agrupa as transações por 'store_nbr' e 'month_day', calculando a média\n",
        "    transactions_grouped = transactions.groupby(['store_nbr', 'month_day']).agg({'transactions': 'mean'}).reset_index()\n",
        "    save_and_cleanup(transactions_grouped, f\"{temp_dir}/transactions_grouped.csv\")\n",
        "\n",
        "    print(\"Loading and saving stores...\")\n",
        "    #Carrega o CSV de lojas\n",
        "    stores = pd.read_csv(stores_path, dtype={'store_nbr': 'int8', 'cluster': 'int8'})\n",
        "    save_and_cleanup(stores, f\"{temp_dir}/stores.csv\")\n",
        "\n",
        "    print(\"Loading and saving holidays_events...\")\n",
        "    #Carrega o CSV de feriados\n",
        "    holidays_events = pd.read_csv(holidays_events_path)\n",
        "    holidays_events['date'] = pd.to_datetime(holidays_events['date'])\n",
        "    save_and_cleanup(holidays_events, f\"{temp_dir}/holidays_events.csv\")\n",
        "\n",
        "    print(\"Loading, processing, and saving oil data...\")\n",
        "    #Carrega o CSV de preços do petróleo ao longo do tempo\n",
        "    oil = pd.read_csv(oil_path)\n",
        "    oil['date'] = pd.to_datetime(oil['date'])\n",
        "    #Completa os preços de petróleo faltantes em algumas datas com interpolação\n",
        "    oil_dates = pd.date_range(start=oil['date'].min(), end=oil['date'].max())\n",
        "    oil_df_full = pd.DataFrame({'date': oil_dates})\n",
        "    oil_df_full = oil_df_full.merge(oil, on='date', how='left')\n",
        "    oil_df_full['dcoilwtico'] = oil_df_full['dcoilwtico'].interpolate()\n",
        "    oil_df_full['dcoilwtico'] = oil_df_full['dcoilwtico'].fillna(method='bfill').fillna(method='ffill')\n",
        "    save_and_cleanup(oil_df_full, f\"{temp_dir}/oil_df_full.csv\")\n",
        "\n",
        "    print(\"Loading data and merging transactions...\")\n",
        "    #Carrega o CSV de dados e converte a coluna 'date' para datetime\n",
        "    merged_df = pd.read_csv(f\"{temp_dir}/data.csv\")\n",
        "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "    merged_df['month_day'] = merged_df['date'].dt.strftime('%m-%d')\n",
        "    #Carrega o CSV de transações agrupadas e converte a coluna 'month_day' para string\n",
        "    transactions_grouped = pd.read_csv(f\"{temp_dir}/transactions_grouped.csv\")\n",
        "    transactions_grouped['month_day'] = transactions_grouped['month_day'].astype(str)\n",
        "    merged_df['month_day'] = merged_df['month_day'].astype(str)\n",
        "    #Mergeia os dados com as transações agrupadas\n",
        "    merged_df = load_and_merge(merged_df, f\"{temp_dir}/transactions_grouped.csv\", on_columns=['store_nbr', 'month_day'], how='left')\n",
        "    merged_df.drop(columns=['month_day'], inplace=True)\n",
        "\n",
        "    save_and_cleanup(merged_df, f\"{temp_dir}/merged_data_transactions.csv\")\n",
        "\n",
        "    print(\"Loading merged data and merging stores...\")\n",
        "    #Carrega o CSV de dados mergeados com transações e converte a coluna 'date' para datetime\n",
        "    merged_df = pd.read_csv(f\"{temp_dir}/merged_data_transactions.csv\")\n",
        "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "    #Mergeia os dados com o CSV de lojas\n",
        "    merged_df = load_and_merge(merged_df, f\"{temp_dir}/stores.csv\", on_columns=['store_nbr'], how='left')\n",
        "\n",
        "    save_and_cleanup(merged_df, f\"{temp_dir}/merged_data_stores.csv\")\n",
        "\n",
        "    print(\"Loading merged data and merging holidays_events...\")\n",
        "    #Carrega o CSV de dados mergeados com lojas e converte a coluna 'date' para datetime\n",
        "    merged_df = pd.read_csv(f\"{temp_dir}/merged_data_stores.csv\")\n",
        "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "    #Mergeia os dados com o CSV de feriados\n",
        "    merged_df = load_and_merge(merged_df, f\"{temp_dir}/holidays_events.csv\", on_columns=['date'], how='left')\n",
        "\n",
        "    save_and_cleanup(merged_df, f\"{temp_dir}/merged_data_holidays_events.csv\")\n",
        "\n",
        "    print(\"Loading merged data and merging oil_df_full...\")\n",
        "    #Carrega o CSV de dados mergeados com feriados e converte a coluna 'date' para datetime\n",
        "    merged_df = pd.read_csv(f\"{temp_dir}/merged_data_holidays_events.csv\")\n",
        "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "    #Mergeia os dados com o CSV de preços de petróleo completo\n",
        "    merged_df = load_and_merge(merged_df, f\"{temp_dir}/oil_df_full.csv\", on_columns=['date'], how='left')\n",
        "    merged_df.rename(columns={'type_x': 'store_type', 'type_y': 'holiday_type'}, inplace=True)\n",
        "\n",
        "    #Adiciona clusters de feriados e outras features adicionais\n",
        "    print(\"Adding holiday clusters and additional features...\")\n",
        "    holiday_clusters = {\n",
        "        'Foundation Days': [\n",
        "            'Fundacion de Manta', 'Fundacion de Cuenca', 'Fundacion de Machala', 'Fundacion de Santo Domingo', 'Fundacion de Esmeraldas',\n",
        "            'Fundacion de Riobamba', 'Fundacion de Ambato', 'Fundacion de Ibarra', 'Fundacion de Quito-1', 'Fundacion de Quito',\n",
        "            'Fundacion de Loja', 'Fundacion de Guayaquil-1', 'Fundacion de Guayaquil', 'Traslado Fundacion de Guayaquil',\n",
        "            'Traslado Fundacion de Quito'\n",
        "        ],\n",
        "        'Provincialization Days': [\n",
        "            'Provincializacion de Cotopaxi', 'Provincializacion de Imbabura', 'Provincializacion de Santo Domingo', 'Provincializacion Santa Elena'\n",
        "        ],\n",
        "        'Cantonization Days': [\n",
        "            'Cantonizacion de Libertad', 'Cantonizacion de Riobamba', 'Cantonizacion del Puyo', 'Cantonizacion de Guaranda', 'Cantonizacion de Latacunga',\n",
        "            'Cantonizacion de El Carmen', 'Cantonizacion de Cayambe', 'Cantonizacion de Quevedo', 'Cantonizacion de Salinas'\n",
        "        ],\n",
        "        'Independence Days': [\n",
        "            'Primer Grito de Independencia', 'Independencia de Guayaquil', 'Independencia de Cuenca', 'Independencia de Guaranda',\n",
        "            'Independencia de Latacunga', 'Independencia de Ambato', 'Traslado Independencia de Guayaquil', 'Traslado Primer Grito de Independencia'\n",
        "        ],\n",
        "        'Religious Holidays': [\n",
        "            'Navidad-4', 'Navidad-3', 'Navidad-2', 'Puente Navidad', 'Navidad-1', 'Navidad', 'Navidad+1', 'Puente Primer dia del ano',\n",
        "            'Primer dia del ano-1', 'Primer dia del ano', 'Viernes Santo'\n",
        "        ],\n",
        "        'Natural Disasters': [\n",
        "            'Terremoto Manabi', 'Terremoto Manabi+1', 'Terremoto Manabi+2', 'Terremoto Manabi+3', 'Terremoto Manabi+4', 'Terremoto Manabi+5',\n",
        "            'Terremoto Manabi+6', 'Terremoto Manabi+7', 'Terremoto Manabi+8', 'Terremoto Manabi+9', 'Terremoto Manabi+10', 'Terremoto Manabi+11',\n",
        "            'Terremoto Manabi+12', 'Terremoto Manabi+13', 'Terremoto Manabi+14', 'Terremoto Manabi+15', 'Terremoto Manabi+16', 'Terremoto Manabi+17',\n",
        "            'Terremoto Manabi+18', 'Terremoto Manabi+19', 'Terremoto Manabi+20', 'Terremoto Manabi+21', 'Terremoto Manabi+22', 'Terremoto Manabi+23',\n",
        "            'Terremoto Manabi+24', 'Terremoto Manabi+25', 'Terremoto Manabi+26', 'Terremoto Manabi+27', 'Terremoto Manabi+28', 'Terremoto Manabi+29',\n",
        "            'Terremoto Manabi+30'\n",
        "        ],\n",
        "        'Public Holidays': [\n",
        "            'Dia de Difuntos', 'Carnaval', 'Dia del Trabajo', 'Dia de la Madre-1', 'Dia de la Madre', 'Puente Dia de Difuntos', 'Recupero Puente Dia de Difuntos', 'Traslado Primer dia del ano'\n",
        "        ],\n",
        "        'Historical Battles and Commemorations': [\n",
        "            'Batalla de Pichincha', 'Traslado Batalla de Pichincha'\n",
        "        ],\n",
        "        'Sports Events': [\n",
        "            'Inauguracion Mundial de futbol Brasil', 'Mundial de futbol Brasil: Ecuador-Suiza', 'Mundial de futbol Brasil: Ecuador-Honduras',\n",
        "            'Mundial de futbol Brasil: Ecuador-Francia', 'Mundial de futbol Brasil: Octavos de Final', 'Mundial de futbol Brasil: Cuartos de Final',\n",
        "            'Mundial de futbol Brasil: Semifinales', 'Mundial de futbol Brasil: Tercer y cuarto lugar', 'Mundial de futbol Brasil: Final'\n",
        "        ],\n",
        "        'Shopping Events': [\n",
        "            'Black Friday', 'Cyber Monday'\n",
        "        ],\n",
        "        'Recovery Days': [\n",
        "            'Recupero puente Navidad', 'Recupero puente primer dia del ano', 'Recupero Puente Navidad', 'Recupero Puente Primer dia del ano'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    #Função para mapear a descrição do feriado para seu cluster correspondente\n",
        "    def map_holiday_cluster(description):\n",
        "        for cluster, holidays in holiday_clusters.items():\n",
        "            if description in holidays:\n",
        "                return cluster\n",
        "        raise ValueError(f\"Unmapped holiday description: {description}\")\n",
        "\n",
        "    #Mapeia a coluna 'description' para o cluster de feriados\n",
        "    merged_df['holiday_cluster'] = merged_df['description'].apply(lambda x: map_holiday_cluster(x) if pd.notna(x) else np.nan)\n",
        "\n",
        "    #Adiciona a coluna 'day_of_week' ao DataFrame\n",
        "    merged_df['day_of_week'] = merged_df['date'].dt.dayofweek\n",
        "\n",
        "    #Adiciona a coluna 'is_weekend' indicando se o dia é fim de semana\n",
        "    merged_df['is_weekend'] = merged_df['day_of_week'].isin([5, 6]).astype(int)  # 5 = Saturday, 6 = Sunday\n",
        "    merged_df['month'] = merged_df['date'].dt.month\n",
        "    merged_df['year'] = merged_df['date'].dt.year\n",
        "    merged_df['week'] = merged_df['date'].dt.isocalendar().week\n",
        "\n",
        "    #Adiciona a coluna 'is_holiday' indicando se o dia é feriado\n",
        "    if 'holiday_type' in merged_df.columns:\n",
        "        merged_df['is_holiday'] = merged_df['holiday_type'].notna().astype(int)\n",
        "    else:\n",
        "        merged_df['is_holiday'] = 0\n",
        "\n",
        "    #Adiciona a coluna 'is_wage_payment_day' indicando se o dia é dia de pagamento de salários\n",
        "    merged_df['is_wage_payment_day'] = merged_df['date'].dt.day.isin(wage_payment_days).astype(int)\n",
        "    #Adiciona a coluna 'earthquake_impact' indicando o impacto do terremoto\n",
        "    merged_df['earthquake_impact'] = ((merged_df['date'] >= earthquake_date) & (merged_df['date'] <= earthquake_date + timedelta(days=30))).astype(int)\n",
        "\n",
        "    #Mapeia a coluna 'family' para seu cluster correspondente\n",
        "    cluster_mapping = {\n",
        "        'AUTOMOTIVE': 'Automotive and Hardware',\n",
        "        'BABY CARE': 'Personal and Home Care',\n",
        "        'BEAUTY': 'Personal and Home Care',\n",
        "        'BEVERAGES': 'Beverages',\n",
        "        'BOOKS': 'Entertainment and Leisure',\n",
        "        'BREAD/BAKERY': 'Food',\n",
        "        'CELEBRATION': 'Celebration and Seasonal',\n",
        "        'CLEANING': 'Personal and Home Care',\n",
        "        'DAIRY': 'Food',\n",
        "        'DELI': 'Food',\n",
        "        'EGGS': 'Food',\n",
        "        'FROZEN FOODS': 'Food',\n",
        "        'GROCERY I': 'Food',\n",
        "        'GROCERY II': 'Food',\n",
        "        'HARDWARE': 'Automotive and Hardware',\n",
        "        'HOME AND KITCHEN I': 'Home and Kitchen',\n",
        "        'HOME AND KITCHEN II': 'Home and Kitchen',\n",
        "        'HOME APPLIANCES': 'Home and Kitchen',\n",
        "        'HOME CARE': 'Personal and Home Care',\n",
        "        'LADIESWEAR': 'Clothing and Accessories',\n",
        "        'LAWN AND GARDEN': 'Celebration and Seasonal',\n",
        "        'LINGERIE': 'Clothing and Accessories',\n",
        "        'LIQUOR,WINE,BEER': 'Beverages',\n",
        "        'MAGAZINES': 'Entertainment and Leisure',\n",
        "        'MEATS': 'Food',\n",
        "        'PERSONAL CARE': 'Personal and Home Care',\n",
        "        'PET SUPPLIES': 'Pet Supplies',\n",
        "        'PLAYERS AND ELECTRONICS': 'Entertainment and Leisure',\n",
        "        'POULTRY': 'Food',\n",
        "        'PREPARED FOODS': 'Food',\n",
        "        'PRODUCE': 'Food',\n",
        "        'SCHOOL AND OFFICE SUPPLIES': 'School and Office Supplies',\n",
        "        'SEAFOOD': 'Food'\n",
        "    }\n",
        "    #Mapeia a coluna 'family' para 'family_cluster'\n",
        "    merged_df['family_cluster'] = merged_df['family'].map(cluster_mapping)\n",
        "    #Remove as colunas 'family' e 'description', já que já temos seus clusters\n",
        "    merged_df = merged_df.drop(columns=['family', 'description'])\n",
        "\n",
        "    #Move a coluna 'sales' para o final do DataFrame\n",
        "    if 'sales' in merged_df.columns:\n",
        "        cols = [col for col in merged_df.columns if col != 'sales'] + ['sales']\n",
        "        merged_df = merged_df[cols]\n",
        "\n",
        "    #O dataframe final é salvo em CSV\n",
        "    save_and_cleanup(merged_df, f\"{temp_dir}/merged_df_final.csv\")\n",
        "\n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>EDA</h3>\n",
        "<p>Realiza análises estatísticas automáticas usando a biblioteca ProfileReport. É uma ferramenta que provê inúmeros insights sobre os dados de maneira rápida e automática.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nk6NqP0xLPPC",
        "outputId": "5a6fa993-a803-481f-a545-c630bcd18f61"
      },
      "outputs": [],
      "source": [
        "#Classe para realizar Análise Exploratória de Dados (EDA)\n",
        "class EDA:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self._preprocess()\n",
        "\n",
        "    #Função para pré-processar os dados\n",
        "    def _preprocess(self):\n",
        "        #Preenche valores nulos nas colunas de feriados\n",
        "        holiday_columns = ['holiday_type', 'locale', 'locale_name', 'transferred', 'holiday_cluster']\n",
        "        for col in holiday_columns:\n",
        "            if col in self.df.columns:\n",
        "                if self.df[col].dtype == 'object':\n",
        "                    self.df[col] = self.df[col].fillna('No_Holiday')\n",
        "                elif self.df[col].dtype == 'bool':\n",
        "                    self.df[col] = self.df[col].fillna(False)\n",
        "                else:\n",
        "                    self.df[col] = self.df[col].fillna(0)\n",
        "\n",
        "        #Preenche valores nulos na coluna transactions com zero\n",
        "        if 'transactions' in self.df.columns:\n",
        "            self.df['transactions'] = self.df['transactions'].fillna(0)\n",
        "\n",
        "        #Substitui os valores numéricos de day_of_week por nomes dos dias\n",
        "        day_of_week_mapping = {0: 'domingo', 1: 'segunda-feira', 2: 'terça-feira', 3: 'quarta-feira',\n",
        "                               4: 'quinta-feira', 5: 'sexta-feira', 6: 'sábado'}\n",
        "        if 'day_of_week' in self.df.columns:\n",
        "            self.df['day_of_week'] = self.df['day_of_week'].map(day_of_week_mapping)\n",
        "\n",
        "        #Substitui os valores numéricos de month por nomes dos meses\n",
        "        month_mapping = {1: 'janeiro', 2: 'fevereiro', 3: 'março', 4: 'abril', 5: 'maio', 6: 'junho',\n",
        "                         7: 'julho', 8: 'agosto', 9: 'setembro', 10: 'outubro', 11: 'novembro', 12: 'dezembro'}\n",
        "        if 'month' in self.df.columns:\n",
        "            self.df['month'] = self.df['month'].map(month_mapping)\n",
        "\n",
        "    #Função para gerar o relatório de EDA\n",
        "    def generate_report(self):\n",
        "        profile = ProfileReport(self.df, title=\"EDA Report\", explorative=True)\n",
        "        profile.to_notebook_iframe()\n",
        "        return profile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_path = 'test.csv'\n",
        "train_path = 'train.csv'\n",
        "transactions_path = 'transactions.csv'\n",
        "stores_path = 'stores.csv'\n",
        "holidays_events_path = 'holidays_events.csv'\n",
        "oil_path = 'oil.csv'\n",
        "parquet_path = 'processed_data.parquet'\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "\n",
        "df_EDA = feature_engineering(train, transactions_path, stores_path, holidays_events_path, oil_path, temp_dir='train_temp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Cria a instância da classe EDA e gera o relatório\n",
        "eda = EDA(df_EDA)\n",
        "report = eda.generate_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C80Gjy1eLjWy"
      },
      "source": [
        "# **Testando alguns modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Preparar os dados para rodar os modelos</h3>\n",
        "<p>Mesmo depois dos dados passarem por feature engineering, eles ainda precisam de algum tratamento para poderem ser utilizados nos modelos. Isso acontece porque muitos modelos não aceitam valores nulos, categóricos ou mesmo valores contínuos cujo range para a mesma feature é muito grande. Como o tratamento de treino e de teste precisam ser feitos da mesma forma, do contrário, causará um erro na hora do treinamento, precisamos padronizar esse tratamento através das funções abaixo </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepara os dados de treino para a modelagem\n",
        "def prepare_data_train(df, target_var, test_size=0.2):\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek  # Adiciona a coluna day_of_week a partir da coluna date\n",
        "\n",
        "    categorical_features = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "    numeric_features = df.select_dtypes(include=[np.number]).columns.drop(target_var)\n",
        "\n",
        "    df[categorical_features] = df[categorical_features].astype(str)\n",
        "\n",
        "    holiday_columns = ['holiday_type', 'locale', 'locale_name', 'transferred', 'holiday_cluster']\n",
        "    for col in holiday_columns:\n",
        "        if col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                df[col] = df[col].fillna('No_Holiday')\n",
        "            elif df[col].dtype == 'bool':\n",
        "                df[col] = df[col].fillna(False)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "    imputer_numeric = SimpleImputer(strategy='mean')\n",
        "    df[numeric_features] = imputer_numeric.fit_transform(df[numeric_features])\n",
        "\n",
        "    # Codificação de variáveis categóricas usando LabelEncoder\n",
        "    encoders = {}\n",
        "    for col in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        encoders[col] = le\n",
        "\n",
        "    X = df.drop(columns=[target_var])\n",
        "    y = df[target_var]\n",
        "\n",
        "    train_size = int(len(X) * (1 - test_size))\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "    assert len(X_train) == len(y_train), \"Tamanho inconsistente entre X_train e y_train\"\n",
        "    assert len(X_test) == len(y_test), \"Tamanho inconsistente entre X_test e y_test\"\n",
        "\n",
        "    train_columns = X.columns  # Captura as colunas do conjunto de treino antes da transformação\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=train_columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), columns=train_columns)\n",
        "\n",
        "    y_scaler = MinMaxScaler()\n",
        "    y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
        "    y_test = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, encoders, imputer_numeric, scaler, y_scaler, train_columns\n",
        "\n",
        "# Prepara os dados de teste para a modelagem\n",
        "def prepare_data_test(df, encoders, imputer_numeric, scaler, train_columns):\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek  # Adiciona a coluna day_of_week a partir da coluna date\n",
        "    categorical_features = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "    numeric_features = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    df[categorical_features] = df[categorical_features].astype(str)\n",
        "\n",
        "    holiday_columns = ['holiday_type', 'locale', 'locale_name', 'transferred', 'holiday_cluster']\n",
        "    for col in holiday_columns:\n",
        "        if col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                df[col] = df[col].fillna('No_Holiday')\n",
        "            elif df[col].dtype == 'bool':\n",
        "                df[col] = df[col].fillna(False)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "    df[numeric_features] = imputer_numeric.transform(df[numeric_features])\n",
        "\n",
        "    for col, encoder in encoders.items():\n",
        "        df[col] = encoder.transform(df[col])\n",
        "\n",
        "    df_encoded = pd.DataFrame(scaler.transform(df), columns=train_columns)\n",
        "\n",
        "    return df_encoded\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Rodando e comparando diferentes modelos usando Optuna</h3>\n",
        "<p>Logo abaixo, 4 modelos serão otimizados (Grid Search com Optuna) e treinados usando uma validação cruzada. Os modelos a serem treinados são: Linear Regression, Random Forest, XGBoost e LightGBM. Depois terão suas métricas 'Root Mean Squared Logarithmic Error' comparadas visualmente </p>\n",
        "<p>Devido a enorme quantidade de tempo que leva para fazer a validação cruzada dos 4 modelos, eu optei pela estratégia de testar as otimizações procurando pela melhor performance e salvar os resultados em disco. Daí se acontecesse algum problema durante a execução, eu não iria precisar retreinar os modelos que já foram treinados, bastando recuperá-los e carregá-los em memória.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Função para calcular o Root Mean Squared Logarithmic Error (RMSLE)def rmsle(y_true, y_pred):\n",
        "def rmsle(y_true, y_pred):\n",
        "    y_true = np.where(y_true <= 0, 0, y_true)\n",
        "    y_pred = np.where(y_pred <= 0, 0, y_pred)\n",
        "    \n",
        "    if not np.all(np.isfinite(y_true)) or not np.all(np.isfinite(y_pred)):\n",
        "        raise ValueError(\"Valores infinitos ou muito grandes encontrados em y_true ou y_pred\")\n",
        "    \n",
        "    log_y_true = np.log1p(y_true)\n",
        "    log_y_pred = np.log1p(y_pred)\n",
        "    \n",
        "    mse = mean_squared_error(log_y_true, log_y_pred)\n",
        "    return np.sqrt(mse)\n",
        "\n",
        "# Função para otimizar os hiperparâmetros usando Optuna\n",
        "def optimize_model(trial, model_name, X_train, y_train):\n",
        "    print(f\"Otimização dos hiperparâmetros para {model_name}...\")\n",
        "    \n",
        "    if model_name == 'Random Forest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
        "        max_depth = trial.suggest_int('max_depth', 3, 50)\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    elif model_name == 'XGBoost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
        "        max_depth = trial.suggest_int('max_depth', 3, 50)\n",
        "        subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
        "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=max_depth,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    elif model_name == 'LightGBM':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
        "        max_bin = trial.suggest_int('max_bin', 100, 300)\n",
        "        bagging_fraction = trial.suggest_float('bagging_fraction', 0.5, 1.0)\n",
        "        feature_fraction = trial.suggest_float('feature_fraction', 0.5, 1.0)\n",
        "        num_leaves = trial.suggest_int('num_leaves', 20, 150)\n",
        "        tweedie_variance_power = trial.suggest_float('tweedie_variance_power', 1.0, 2.0)\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_bin=max_bin,\n",
        "            bagging_fraction=bagging_fraction,\n",
        "            feature_fraction=feature_fraction,\n",
        "            num_leaves=num_leaves,\n",
        "            tweedie_variance_power=tweedie_variance_power,\n",
        "            objective='tweedie',\n",
        "            random_state=42,\n",
        "            force_row_wise=True,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    scores = []\n",
        "\n",
        "    if isinstance(X_train, pd.DataFrame):\n",
        "        X_train = X_train.to_numpy()\n",
        "    if isinstance(y_train, pd.Series):\n",
        "        y_train = y_train.to_numpy()\n",
        "\n",
        "    for train_idx, val_idx in tqdm(tscv.split(X_train), desc=f\"Cross-validation para {model_name}\"):\n",
        "        X_t, X_v = X_train[train_idx], X_train[val_idx]\n",
        "        y_t, y_v = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        model.fit(X_t, y_t)\n",
        "        y_pred = model.predict(X_v)\n",
        "        score = np.sqrt(mean_squared_error(y_v, y_pred))\n",
        "        scores.append(score)\n",
        "\n",
        "    mean_score = np.mean(scores)\n",
        "    return mean_score\n",
        "\n",
        "# Função para plotar a comparação de desempenho dos modelos\n",
        "def plot_model_comparisons(model_results):\n",
        "    model_names = list(model_results.keys())\n",
        "    rmsle_values = list(model_results.values())\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=model_names, y=rmsle_values, palette=\"viridis\")\n",
        "    plt.xlabel('Modelos')\n",
        "    plt.ylabel('RMSLE')\n",
        "    plt.title('Comparação de Desempenho dos Modelos')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Função para plotar as previsões do modelo\n",
        "def plot_predictions(y_test, y_pred, model_name, y_scaler, dates):\n",
        "    y_test = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "    y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "    df_y_test = pd.DataFrame({'date': dates, 'y_test': y_test})\n",
        "    df_y_pred = pd.DataFrame({'date': dates, 'y_pred': y_pred})\n",
        "\n",
        "    df_y_test_daily = df_y_test.groupby('date').mean().reset_index()\n",
        "    df_y_pred_daily = df_y_pred.groupby('date').mean().reset_index()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df_y_test_daily['date'], df_y_test_daily['y_test'], label='Real')\n",
        "    plt.plot(df_y_pred_daily['date'], df_y_pred_daily['y_pred'], label='Previsto')\n",
        "    plt.title(f'Previsões com {model_name}')\n",
        "    plt.xlabel('Data')\n",
        "    plt.ylabel('Vendas Médias Diárias')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Função para executar os modelos otimizados e comparar suas performances\n",
        "def run_optimized_models(df, target_var='sales', save_dir='model_results'):\n",
        "    # Cria o diretório para salvar os resultados dos modelos se não existir\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Preparando os dados...\")\n",
        "    #Prepara os dados de treino e teste\n",
        "    X_train, X_test, y_train, y_test, encoders, imputer_numeric, scaler, y_scaler, train_columns = prepare_data_train(df, target_var)\n",
        "    #Extrai as datas para o conjunto de teste\n",
        "    dates = df.iloc[X_test.index]['date']  \n",
        "\n",
        "    print(\"Verificando colunas após preparação dos dados...\")\n",
        "    print(f\"Colunas do conjunto de treino: {list(X_train.columns)}\")\n",
        "    print(f\"Colunas do conjunto de teste: {list(X_test.columns)}\")\n",
        "\n",
        "    print(\"Iniciando otimização dos hiperparâmetros...\")\n",
        "    #Definições dos modelos e suas funções de otimização\n",
        "    model_definitions = {\n",
        "        'Random Forest': ('study_rf.pkl', lambda trial: optimize_model(trial, 'Random Forest', X_train, y_train)),\n",
        "        'XGBoost': ('study_xgb.pkl', lambda trial: optimize_model(trial, 'XGBoost', X_train, y_train)),\n",
        "        'LightGBM': ('study_lgbm.pkl', lambda trial: optimize_model(trial, 'LightGBM', X_train, y_train))\n",
        "    }\n",
        "\n",
        "    best_params = {}\n",
        "    for model_name, (file_name, opt_function) in model_definitions.items():\n",
        "        model_path = os.path.join(save_dir, file_name)\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"{model_name} já foi otimizado. Pulando para o próximo.\")\n",
        "            with open(model_path, 'rb') as f:\n",
        "                study = pickle.load(f)\n",
        "            best_params[model_name] = study.best_params\n",
        "        else:\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            try:\n",
        "                study.optimize(opt_function, n_trials=10)\n",
        "                best_params[model_name] = study.best_params\n",
        "                with open(model_path, 'wb') as f:\n",
        "                    pickle.dump(study, f)\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao otimizar {model_name}: {e}\")\n",
        "                with open(os.path.join(save_dir, f'{model_name}_optimization_error.log'), 'w') as f:\n",
        "                    f.write(traceback.format_exc())\n",
        "                raise e\n",
        "\n",
        "    #Inicializa os modelos com os melhores hiperparâmetros encontrados\n",
        "    models = {\n",
        "        'Regressão Linear': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(**best_params['Random Forest'], random_state=42, n_jobs=-1),\n",
        "        'XGBoost': XGBRegressor(**best_params['XGBoost'], random_state=42, n_jobs=-1),\n",
        "        'LightGBM': LGBMRegressor(**best_params['LightGBM'], random_state=42, n_jobs=-1, force_row_wise=True)\n",
        "    }\n",
        "\n",
        "    model_results = {}\n",
        "\n",
        "    #Treina e avalia cada modelo\n",
        "    for model_name, model in tqdm(models.items(), desc=\"Treinando modelos\"):\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred = np.maximum(y_pred, 0)\n",
        "            rmsle_value = rmsle(y_test, y_pred)\n",
        "            print(f'{model_name} RMSLE: {rmsle_value}')\n",
        "            model_results[model_name] = rmsle_value\n",
        "            plot_predictions(y_test, y_pred, model_name, y_scaler, dates)\n",
        "\n",
        "            #Salva o modelo treinado e as previsões\n",
        "            with open(os.path.join(save_dir, f'{model_name}_model.pkl'), 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            np.save(os.path.join(save_dir, f'{model_name}_y_pred.npy'), y_pred)\n",
        "            np.save(os.path.join(save_dir, f'{model_name}_y_test.npy'), y_test)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao treinar {model_name}: {e}\")\n",
        "            with open(os.path.join(save_dir, f'{model_name}_training_error.log'), 'w') as f:\n",
        "                f.write(traceback.format_exc())\n",
        "            raise e\n",
        "\n",
        "    #Plota a comparação de desempenho dos modelos\n",
        "    plot_model_comparisons(model_results)\n",
        "    with open(os.path.join(save_dir, 'model_results.pkl'), 'wb') as f:\n",
        "        pickle.dump(model_results, f)\n",
        "\n",
        "    return model_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Criação do modelo final utilizando os resultados do passo anterior</h3>\n",
        "<p>Depois dos modelos terem sido criados e otimizados, irei criar um modelo final, que é o modelo que será usado para submeter os dados ao Kaggle. Também irei comparar visualmente o pior modelo encontrado com o melhor modelo encontrado em relação à performance com a métrica Root Mean Squared Logarithmic Error</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para criar o modelo final\n",
        "def cria_modelo_final(df, target_var='sales', save_dir='model_results'):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Preparando os dados...\")\n",
        "    X_train, X_test, y_train, y_test, encoders, imputer_numeric, scaler, y_scaler, train_columns = prepare_data_train(df, target_var, test_size=0.2)\n",
        "\n",
        "    # Extrai as datas para o conjunto de teste\n",
        "    dates = df.iloc[X_test.index]['date']  \n",
        "\n",
        "    print(\"Treinando a baseline (Regressão Linear)...\")\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    y_pred_lr = lr_model.predict(X_test)\n",
        "    y_pred_lr = np.maximum(y_pred_lr, 0)\n",
        "    lr_rmsle = rmsle(y_test, y_pred_lr)\n",
        "\n",
        "    with open(os.path.join(save_dir, 'linear_regression_model.pkl'), 'wb') as f:\n",
        "        pickle.dump(lr_model, f)\n",
        "\n",
        "    print(f'Regressão Linear RMSLE: {lr_rmsle}')\n",
        "\n",
        "    best_rmsle = lr_rmsle\n",
        "    best_model = lr_model\n",
        "    best_model_name = 'linear_regression_model.pkl'\n",
        "\n",
        "    for model_file in os.listdir(save_dir):\n",
        "        if model_file.endswith('_model.pkl') and model_file != 'linear_regression_model.pkl':\n",
        "            model_path = os.path.join(save_dir, model_file)\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "\n",
        "            X_test_reordered = X_test[train_columns]\n",
        "\n",
        "            y_pred_model = model.predict(X_test_reordered)\n",
        "            y_pred_model = np.maximum(y_pred_model, 0)\n",
        "            model_rmsle = rmsle(y_test, y_pred_model)\n",
        "            print(f'{model_file} RMSLE: {model_rmsle}')\n",
        "\n",
        "            if model_rmsle < best_rmsle:\n",
        "                best_rmsle = model_rmsle\n",
        "                best_model = model\n",
        "                best_model_name = model_file\n",
        "\n",
        "    print(f'O melhor modelo é: {best_model_name} com RMSLE: {best_rmsle}')\n",
        "    plot_predictions(y_test, best_model.predict(X_test[train_columns]), best_model_name, y_scaler, dates)\n",
        "\n",
        "    return best_model, encoders, imputer_numeric, scaler, y_scaler, train_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Criando o arquivo de submissão</h1>\n",
        "<p>Com o modelo final criado, basta apenas criar o arquivo de submissão para o Kaggle</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_and_submit(model, df_test, test, encoders, imputer_numeric, scaler, y_scaler, train_columns):\n",
        "    df_test_prepared = prepare_data_test(df_test, encoders, imputer_numeric, scaler, train_columns)\n",
        "\n",
        "    # Garantir que as colunas estejam na mesma ordem que as colunas de treinamento\n",
        "    df_test_prepared = df_test_prepared[train_columns]\n",
        "\n",
        "    # Realiza as previsões\n",
        "    predictions = model.predict(df_test_prepared)\n",
        "    predictions = y_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "\n",
        "    # Cria e salva o arquivo de submissão\n",
        "    output_filename = 'regression_submission.csv'\n",
        "    submission = pd.DataFrame({'id': test['id'], 'sales': predictions})\n",
        "    submission.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"Submission file created: {output_filename}\")\n",
        "    return submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Executando todas as funções que foram criadas previamente</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing initial data...\n",
            "Saved and cleaned up: train_temp/data.csv\n",
            "Loading and saving transactions...\n",
            "Saved and cleaned up: train_temp/transactions_grouped.csv\n",
            "Loading and saving stores...\n",
            "Saved and cleaned up: train_temp/stores.csv\n",
            "Loading and saving holidays_events...\n",
            "Saved and cleaned up: train_temp/holidays_events.csv\n",
            "Loading, processing, and saving oil data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_27052\\3176306971.py:83: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  oil_df_full['dcoilwtico'] = oil_df_full['dcoilwtico'].fillna(method='bfill').fillna(method='ffill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved and cleaned up: train_temp/oil_df_full.csv\n",
            "Loading data and merging transactions...\n",
            "Merged with: train_temp/transactions_grouped.csv\n",
            "Saved and cleaned up: train_temp/merged_data_transactions.csv\n",
            "Loading merged data and merging stores...\n",
            "Merged with: train_temp/stores.csv\n",
            "Saved and cleaned up: train_temp/merged_data_stores.csv\n",
            "Loading merged data and merging holidays_events...\n",
            "Merged with: train_temp/holidays_events.csv\n",
            "Saved and cleaned up: train_temp/merged_data_holidays_events.csv\n",
            "Loading merged data and merging oil_df_full...\n",
            "Merged with: train_temp/oil_df_full.csv\n",
            "Adding holiday clusters and additional features...\n",
            "Saved and cleaned up: train_temp/merged_df_final.csv\n",
            "Loading and preprocessing initial data...\n",
            "Saved and cleaned up: test_temp/data.csv\n",
            "Loading and saving transactions...\n",
            "Saved and cleaned up: test_temp/transactions_grouped.csv\n",
            "Loading and saving stores...\n",
            "Saved and cleaned up: test_temp/stores.csv\n",
            "Loading and saving holidays_events...\n",
            "Saved and cleaned up: test_temp/holidays_events.csv\n",
            "Loading, processing, and saving oil data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_27052\\3176306971.py:83: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  oil_df_full['dcoilwtico'] = oil_df_full['dcoilwtico'].fillna(method='bfill').fillna(method='ffill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved and cleaned up: test_temp/oil_df_full.csv\n",
            "Loading data and merging transactions...\n",
            "Merged with: test_temp/transactions_grouped.csv\n",
            "Saved and cleaned up: test_temp/merged_data_transactions.csv\n",
            "Loading merged data and merging stores...\n",
            "Merged with: test_temp/stores.csv\n",
            "Saved and cleaned up: test_temp/merged_data_stores.csv\n",
            "Loading merged data and merging holidays_events...\n",
            "Merged with: test_temp/holidays_events.csv\n",
            "Saved and cleaned up: test_temp/merged_data_holidays_events.csv\n",
            "Loading merged data and merging oil_df_full...\n",
            "Merged with: test_temp/oil_df_full.csv\n",
            "Adding holiday clusters and additional features...\n",
            "Saved and cleaned up: test_temp/merged_df_final.csv\n"
          ]
        }
      ],
      "source": [
        "test_path = 'test.csv'\n",
        "train_path = 'train.csv'\n",
        "transactions_path = 'transactions.csv'\n",
        "stores_path = 'stores.csv'\n",
        "holidays_events_path = 'holidays_events.csv'\n",
        "oil_path = 'oil.csv'\n",
        "parquet_path = 'processed_data.parquet'\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "filtered_train = train[train['date'] >= \"2017-01-01\"].copy()\n",
        "filtered_train = filtered_train[filtered_train['family'].isin(test['family'])].copy()\n",
        "\n",
        "df_transformed = feature_engineering(filtered_train, transactions_path, stores_path, holidays_events_path, oil_path, temp_dir='train_temp')\n",
        "df_test = feature_engineering(test, transactions_path, stores_path, holidays_events_path, oil_path, temp_dir='test_temp')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando os dados...\n"
          ]
        },
        {
          "ename": "DTypePromotionError",
          "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_optimized_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_transformed\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[5], line 131\u001b[0m, in \u001b[0;36mrun_optimized_models\u001b[1;34m(df, target_var, save_dir)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparando os dados...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#Prepara os dados de treino e teste\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m X_train, X_test, y_train, y_test, encoders, imputer_numeric, scaler, y_scaler, train_columns \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#Extrai as datas para o conjunto de teste\u001b[39;00m\n\u001b[0;32m    133\u001b[0m dates \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[X_test\u001b[38;5;241m.\u001b[39mindex][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n",
            "Cell \u001b[1;32mIn[4], line 43\u001b[0m, in \u001b[0;36mprepare_data_train\u001b[1;34m(df, target_var, test_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m train_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# Captura as colunas do conjunto de treino antes da transformação\u001b[39;00m\n\u001b[0;32m     42\u001b[0m scaler \u001b[38;5;241m=\u001b[39m RobustScaler()\n\u001b[1;32m---> 43\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mtrain_columns)\n\u001b[0;32m     44\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mtrain_columns)\n\u001b[0;32m     46\u001b[0m y_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1591\u001b[0m, in \u001b[0;36mRobustScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the median and quantiles to be used for scaling.\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \n\u001b[0;32m   1575\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;66;03m# at fit, convert sparse matrices to csc for optimized computation of\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;66;03m# the quantiles\u001b[39;00m\n\u001b[1;32m-> 1591\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1598\u001b[0m q_min, q_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantile_range\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_min \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32md:\\POS_GRADUACAO\\neoenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:882\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    878\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    879\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 882\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
            "\u001b[1;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)"
          ]
        }
      ],
      "source": [
        "model_results = run_optimized_models(df_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar os dados de treinamento e criar o modelo final\n",
        "best_model, encoders, imputer_numeric, scaler, y_scaler, train_columns = cria_modelo_final(df_transformed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar os dados de teste e criar a submissão\n",
        "submission = prepare_and_submit(best_model, df_test, test, encoders, imputer_numeric, scaler, y_scaler, train_columns)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
