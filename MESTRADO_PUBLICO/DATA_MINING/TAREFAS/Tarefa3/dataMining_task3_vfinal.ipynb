{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Objetivo</h1>\n",
        "<h5><b>Avaliação:</b></h5><p>Para avaliar a performance da previsão do timestamp datahora, será usada a métrica RMSE. Para avaliar a performance da previsão de latitude e longitude, será usada a métrica Mean Haversine Distance</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Manipulação de arquivos\n",
        "import os\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "#Manipulação e processamento de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cudf\n",
        "import cupy as cp\n",
        "\n",
        "#Machine Learning\n",
        "from cuml.cluster import DBSCAN as cuDBSCAN\n",
        "from sklearn.cluster import DBSCAN as SklearnDBSCAN\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import torch\n",
        "import joblib\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "#Barra de progresso\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "#Visualização\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "#Perfil de memória\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "#Configurações de de visualização no Pandas\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#Suprimir mensagens de warning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*fillna with 'method' is deprecated.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Could not infer format, so each element will be parsed individually.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"'squared' is deprecated\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*The tree method `gpu_hist` is deprecated since 2.0.0.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Falling back to prediction using DMatrix due to mismatched devices.*\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpE2fEW2O6nD"
      },
      "outputs": [],
      "source": [
        "#Caminhos das pastas\n",
        "base_dir = os.getcwd()\n",
        "dados_dir = os.path.join(base_dir, 'dados')\n",
        "intermediarios_dir = os.path.join(dados_dir, 'intermediarios')\n",
        "processados_dir = os.path.join(dados_dir, 'processados')\n",
        "\n",
        "#Criar diretórios se não existirem\n",
        "os.makedirs(intermediarios_dir, exist_ok=True)\n",
        "os.makedirs(processados_dir, exist_ok=True)\n",
        "\n",
        "#Lista global para armazenar os DataFrames processados\n",
        "final_df_list = []\n",
        "\n",
        "#Dicionário para armazenar o sentido atual de cada ônibus\n",
        "sentidos_atual = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>EDA</h1>\n",
        "<p>Fazendo algumas análises estatísticas e plotagens de dados</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EDA:\n",
        "    def __init__(self, file_path):\n",
        "        #Inicializa a classe com o caminho do arquivo de dados processados.\n",
        "        self.file_path = file_path  \n",
        "\n",
        "    #Carrega os dados do arquivo Parquet usando pandas ou cudf.\n",
        "    def load_data(self, use_cudf=False):\n",
        "        try:\n",
        "            if use_cudf:\n",
        "                return cudf.read_parquet(self.file_path) \n",
        "            else:\n",
        "                return pd.read_parquet(self.file_path) \n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao carregar os dados: {e}\")\n",
        "            return None\n",
        "\n",
        "    #Realiza uma inspeção inicial dos dados, visualizando as primeiras linhas e um resumo estatístico.\n",
        "    def initial_inspection(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            print(df.describe()) \n",
        "            del df  \n",
        "            gc.collect() \n",
        "\n",
        "    #Analisa as estatísticas dos dados, incluindo a distribuição das velocidades e a contagem de registros por linha e sentido.\n",
        "    def analyze_statistics(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Plota a distribuição das velocidades\n",
        "            df['velocidade'].hist(bins=50)  # Plota um histograma\n",
        "            plt.title('Distribuição de Velocidades')\n",
        "            plt.xlabel('Velocidade')\n",
        "            plt.ylabel('Frequência')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "            plt.show()\n",
        "\n",
        "            #Conta registros por linha e sentido e exibe os resultados\n",
        "            #print(df.groupby(['linha', 'sentido']).size().to_frame('count').reset_index())\n",
        "            del df \n",
        "            gc.collect()\n",
        "\n",
        "    #Analisa padrões temporais dos dados, incluindo o volume de dados por hora e padrões de movimentação ao longo do tempo.\n",
        "    def analyze_temporal_patterns(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Extrai a hora do timestamp\n",
        "            #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "            df['hora'] = df['datahora_converted'].dt.hour\n",
        "\n",
        "            # Filtra as horas entre 8 e 22\n",
        "            df = df[(df['hora'] >= 8) & (df['hora'] < 22)]\n",
        "\n",
        "            #Conta registros por hora e plota o volume de dados por hora\n",
        "            hourly_counts = df.groupby('hora').size().to_frame('count').reset_index()\n",
        "            hourly_counts.plot(x='hora', y='count', kind='bar')\n",
        "            plt.title('Volume de Dados por Hora')\n",
        "            plt.xlabel('Hora do Dia')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.show()\n",
        "\n",
        "            #Conta registros por dia ao longo do tempo e plota padrões de movimentação\n",
        "            df['data'] = df['datahora_converted'].dt.date  # Extrai apenas a data\n",
        "            daily_counts = df.groupby('data').size().to_frame('count').reset_index()\n",
        "            daily_counts.plot(x='data', y='count', kind='line')\n",
        "            plt.title('Padrões de Movimentação ao Longo do Tempo')\n",
        "            plt.xlabel('Data')\n",
        "            plt.ylabel('Contagem')\n",
        "            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Formata o eixo x para datas\n",
        "            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.show()\n",
        "\n",
        "            del df  \n",
        "            gc.collect()  \n",
        "\n",
        "    #Analisa padrões geográficos dos dados, plotando um mapa de calor dos trajetos dos ônibus.\n",
        "    def analyze_geographical_patterns(self, linhas=None):\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                #Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                #Mostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                #Dados para o mapa de calor\n",
        "                heat_data = [[row['latitude'], row['longitude']] for index, row in df_pandas[['latitude', 'longitude']].iterrows()]\n",
        "                HeatMap(heat_data).add_to(map_osm)  # Adiciona o mapa de calor ao mapa\n",
        "\n",
        "                title = 'Mapa de Calor de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Calor das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de calor salvo como 'bus_routes.html' com {len(df_pandas)} pontos de amostra\")\n",
        "                \n",
        "                del df_cudf  \n",
        "                del df_pandas  \n",
        "                gc.collect()\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de calor: {e}\")\n",
        "\n",
        "    #Plota os trajetos dos ônibus, locais de garagem e pontos finais com diferentes cores.\n",
        "    def plot_routes_and_endpoints(self, linhas=None):\n",
        "        try:\n",
        "            df_cudf = self.load_data(use_cudf=True)\n",
        "            if df_cudf is not None:\n",
        "                df_cudf['linha'] = df_cudf['linha'].astype(str)  # Garante que a coluna 'linha' seja string\n",
        "\n",
        "                if linhas:\n",
        "                    linhas = list(map(str, linhas))\n",
        "                    for linha in linhas:\n",
        "                        if linha not in df_cudf['linha'].to_pandas().tolist():\n",
        "                            raise ValueError(f\"Linha {linha} não encontrada nos dados.\")\n",
        "\n",
        "                    df_cudf = df_cudf[df_cudf['linha'].isin(linhas)]\n",
        "\n",
        "                num_linhas = len(linhas) if linhas else len(df_cudf['linha'].unique())\n",
        "                sample_fraction = min(1.0 / num_linhas, 0.01)  # Ajusta a fração de amostra de acordo com o número de linhas\n",
        "                max_points = min(5000 * num_linhas, 50000)  # Ajusta o número máximo de pontos de acordo com o número de linhas\n",
        "\n",
        "                #Define o centro do mapa baseado na média de latitude e longitude usando cudf\n",
        "                map_center = [float(df_cudf['latitude'].mean()), float(df_cudf['longitude'].mean())]\n",
        "                map_osm = folium.Map(location=map_center, zoom_start=12)  # Cria um mapa centrado na localização média\n",
        "\n",
        "                #Mostra os dados antes de converter para pandas para evitar problemas de memória\n",
        "                if len(df_cudf) > max_points:\n",
        "                    df_sample = df_cudf.sample(frac=sample_fraction)\n",
        "                    if len(df_sample) > max_points:\n",
        "                        df_sample = df_sample.sample(n=max_points)\n",
        "                else:\n",
        "                    df_sample = df_cudf\n",
        "\n",
        "                df_pandas = df_sample.to_pandas()\n",
        "\n",
        "                #Plota os trajetos normais\n",
        "                normal_routes = df_pandas[~df_pandas['garagem'] & ~df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in normal_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Trajetos Normais', gradient={0.4: 'blue', 1: 'lightblue'}).add_to(map_osm)\n",
        "\n",
        "                #Plota os locais de garagem\n",
        "                garage_routes = df_pandas[df_pandas['garagem']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in garage_routes[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Locais de Garagem', gradient={0.4: 'red', 1: 'darkred'}).add_to(map_osm)\n",
        "\n",
        "                #Plota os pontos finais\n",
        "                end_points = df_pandas[df_pandas['ponto_final']]\n",
        "                HeatMap([[row['latitude'], row['longitude']] for index, row in end_points[['latitude', 'longitude']].iterrows()],\n",
        "                        name='Pontos Finais', gradient={0.4: 'yellow', 1: 'gold'}).add_to(map_osm)\n",
        "\n",
        "                title = 'Mapa de Trajetos, Locais de Garagem e Pontos Finais de Todas as Linhas'\n",
        "                if linhas:\n",
        "                    title = f'Mapa de Trajetos, Locais de Garagem e Pontos Finais das Linhas: {\", \".join(map(str, linhas))}'\n",
        "                map_osm.get_root().html.add_child(folium.Element(f\"<h1>{title}</h1>\"))\n",
        "\n",
        "                map_osm.save('bus_routes_endpoints.html')  # Salva o mapa como um arquivo HTML\n",
        "                print(f\"Mapa de trajetos e pontos salvos como 'bus_routes_endpoints.html' com {len(df_pandas)} pontos de amostra\")\n",
        "\n",
        "                del df_cudf  #Libera a memória\n",
        "                del df_pandas  #Libera a memória\n",
        "                gc.collect()  #Coleta o lixo\n",
        "            else:\n",
        "                print(\"Erro ao carregar os dados.\")\n",
        "        except ValueError as ve:\n",
        "            print(ve)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar o mapa de trajetos e pontos: {e}\")\n",
        "\n",
        "    #Analisa a distribuição dos pontos finais e pontos de garagem.\n",
        "    def analyze_endpoints_garage(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            #Plota pontos finais\n",
        "            df[df['ponto_final'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos Finais')\n",
        "            plt.show()\n",
        "\n",
        "            #Plota pontos de garagem\n",
        "            df[df['garagem'] == True][['latitude', 'longitude']].plot(kind='scatter', x='longitude', y='latitude')\n",
        "            plt.title('Distribuição de Pontos de Garagem')\n",
        "            plt.show()\n",
        "\n",
        "            del df  #Libera a memória\n",
        "            gc.collect()  #Coleta o lixo\n",
        "\n",
        "    #Analisa os trajetos dos ônibus para cada linha.\n",
        "    def analyze_routes(self):\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            linhas = df['linha'].unique()  # Obtém todas as linhas únicas\n",
        "            num_linhas = len(linhas)\n",
        "            num_cols = 3\n",
        "            num_rows = (num_linhas + num_cols - 1) // num_cols  # Calcula o número de linhas para os subplots\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            for idx, linha in enumerate(linhas):\n",
        "                trajeto = df[df['linha'] == linha]  # Filtra os dados pela linha\n",
        "                ax = axes[idx]\n",
        "                ax.plot(trajeto['longitude'], trajeto['latitude'], label=f'Linha {linha}')  # Plota o trajeto\n",
        "                ax.set_title(f'Trajeto da Linha {linha}')\n",
        "                ax.set_xlabel('Longitude')\n",
        "                ax.set_ylabel('Latitude')\n",
        "                ax.legend()\n",
        "                ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo y\n",
        "                ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))  # Formata o eixo x\n",
        "\n",
        "            # Remove subplots vazios\n",
        "            for idx in range(len(linhas), len(axes)):\n",
        "                fig.delaxes(axes[idx])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def check_data_quality(self):\n",
        "        \"\"\"\n",
        "        Verifica a qualidade dos dados, incluindo valores nulos, dados inconsistentes e outliers.\n",
        "        \"\"\"\n",
        "        df = self.load_data(use_cudf=False)\n",
        "        if df is not None:\n",
        "            # Verifica valores nulos em cada coluna\n",
        "            print(\"Verificando valores nulos...\")\n",
        "            print(df.isnull().sum())\n",
        "\n",
        "            # Verifica valores não numéricos na coluna 'velocidade'\n",
        "            print(\"Verificando dados inconsistentes...\")\n",
        "            problematic_values = df[~df['velocidade'].astype(str).str.isnumeric()]\n",
        "            if not problematic_values.empty:\n",
        "                print(f\"Valores problemáticos em 'velocidade':\\n{problematic_values[['velocidade', 'velocidade']].head()}\")\n",
        "\n",
        "            # Verifica outliers de velocidade\n",
        "            print(\"Verificando outliers de velocidade [Velocidade acima de 150km/h]...\")\n",
        "            outliers = df[(df['velocidade'] < 0) | (df['velocidade'] > 150)]\n",
        "            print(f\"Outliers:\\n{outliers}\")\n",
        "\n",
        "            del df  # Libera a memória\n",
        "            gc.collect()  # Coleta o lixo\n",
        "\n",
        "    def feature_engineering(self, df):\n",
        "        \"\"\"\n",
        "        Cria novas features a partir das informações temporais.\n",
        "        \"\"\"\n",
        "        print(\"Criando novas features...\")\n",
        "        #df['datahora_converted'] = pd.to_datetime(df['datahora'])\n",
        "        df['dia_da_semana'] = df['datahora_converted'].dt.dayofweek  # Adiciona a feature 'dia_da_semana'\n",
        "        df['hora'] = df['datahora_converted'].dt.hour  # Adiciona a feature 'hora'\n",
        "        print(\"Novas features criadas: 'dia_da_semana', 'hora'\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def run_all(self):\n",
        "        \"\"\"\n",
        "        Executa todos os métodos de análise exploratória de dados em sequência.\n",
        "        \"\"\"\n",
        "        self.initial_inspection()\n",
        "        self.analyze_statistics()\n",
        "        self.analyze_temporal_patterns()\n",
        "        self.analyze_geographical_patterns()\n",
        "        self.plot_routes_and_endpoints()\n",
        "        self.analyze_endpoints_garage()\n",
        "        self.analyze_routes()\n",
        "        self.check_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eda = EDA(file_path=os.path.join(processados_dir, 'processed_data.parquet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Realizar inspeção inicial\n",
        "eda.initial_inspection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar estatísticas\n",
        "eda.analyze_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar padrões temporais\n",
        "eda.analyze_temporal_patterns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar padrões geográficos\n",
        "eda.analyze_geographical_patterns(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotar trajetos e pontos\n",
        "eda.plot_routes_and_endpoints(linhas=[\"324\", \"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar pontos finais e pontos de garagem\n",
        "eda.analyze_endpoints_garage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Analisar trajetos dos ônibus\n",
        "eda.analyze_routes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Verificar qualidade dos dados\n",
        "eda.check_data_quality()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Treinando 2 horas de dados consecutivas para prever a próxima hora</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Extraindo os dados dos arquivos .zip e concatenando os arquivos .json:</h3>\n",
        "<p>Nessa parte do código a ideia é extrair os arquivos .json dos arquivos .zip. Cada arquivo .zip contém 2 tipos de dados: dados históricos e dados de teste. Portanto, ao final, todos os arquivos de dados históricos foram concatenados em 1 arquivo e todos os arquivos de teste foram concatenados em outro arquivo.</p>\n",
        "<h3>2 tipos de arquivos de teste:</h3>\n",
        "<p>Existem 2 arquivos de teste: Um para testar previsão de datahora e outro para testar a previsão de latitude e longitude. Portanto, 2 arquivos de teste foram criados</p>\n",
        "<h3>Filtragem Inicial:</h3>\n",
        "<p>Para atender as demandas do enunciado do professor Zimbrão, algumas filtragens iniciais foram feitas:\n",
        "<li>apenas linhas específicas que serão analizadas</li>\n",
        "<li>considerei apenas os limites de velocidade entre 0 e 250</li>\n",
        "<li>considerei apenas os limites de latitude e longitude possíveis: entre -90 e 90 e entre -180 e 180</li>\n",
        "<li>eu adicionei os 2 últimos dígitos de cada arquivo .json em uma nova coluna para ambos os datasets de treino e teste e chamei essa nova coluna de hour_from_file. O objetivo é usar essa coluna como referência na hora de treinar as 2 últimas horas para prever a hora seguinte</li>\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "A classe `FinalDataProcessor` é responsável por processar arquivos de dados finais. Ela define o diretório \n",
        "base e cria diretórios para arquivos intermediários e processados, além de definir um tamanho de chunk \n",
        "padrão.\n",
        "'''\n",
        "class FinalDataProcessor:\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = os.path.join(base_dir, 'dados_finais')\n",
        "        self.intermediarios_teste_dir = os.path.join(self.base_dir, 'intermediarios_teste')\n",
        "        self.processados_teste_dir = os.path.join(self.base_dir, 'processados_teste')\n",
        "        self.chunk_size = 100000\n",
        "\n",
        "        os.makedirs(self.intermediarios_teste_dir, exist_ok=True)\n",
        "        os.makedirs(self.processados_teste_dir, exist_ok=True)\n",
        "\n",
        "    '''\n",
        "    Recebe uma lista de arquivos zip e extrai cada um deles no diretório de \n",
        "    arquivos intermediários de teste. Em seguida, processa os arquivos JSON extraídos.\n",
        "    '''\n",
        "    def extract_zip_files(self, zip_files):\n",
        "        '''\n",
        "        Loop através de cada arquivo zip na lista `zip_files`.\n",
        "        '''\n",
        "        for zip_file in tqdm(zip_files, desc=\"Extracting zip files\"):\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.intermediarios_teste_dir)\n",
        "                date_from_file = '-'.join(os.path.basename(zip_file).split('-')[1:4]).split('.')[0]\n",
        "                print(f\"DATE FROM FILE: {date_from_file}\")\n",
        "                self.process_json_files(date_from_file)\n",
        "\n",
        "    '''\n",
        "    Chama os métodos para processar e salvar arquivos de teste e concatenar e salvar arquivos de treino, \n",
        "    com base na data extraída do nome do arquivo.\n",
        "    '''\n",
        "    def process_json_files(self, date_from_file):\n",
        "        self.process_and_save_test_files(date_from_file)\n",
        "        self.concatenate_and_save_train_files(date_from_file)\n",
        "\n",
        "    '''\n",
        "    Lê um arquivo JSON e o converte em um DataFrame do cuDF. cuDF é usado para acelerar o processo, usando\n",
        "    a aceleração por GPU\n",
        "    '''\n",
        "    def process_json(self, json_path):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        df = cudf.DataFrame.from_pandas(pd.DataFrame(data))\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Faz o pré-processamento do DataFrame de treino, adicionando colunas com informações de data e hora \n",
        "    extraídas do nome do arquivo, filtrando linhas válidas e convertendo e interpolando colunas específicas.\n",
        "    '''\n",
        "    def preprocess_train_df(self, df, json_path, date_from_file):\n",
        "        hour_from_file_train = int(json_path.split('_')[-1].split('.')[0])\n",
        "        df['hour_from_file_train'] = hour_from_file_train\n",
        "        df['date_hour_from_file_train'] = f\"{date_from_file}_{hour_from_file_train}\"\n",
        "        df['date_from_file'] = date_from_file\n",
        "\n",
        "        linhas_validas = [483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, 550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, 557, 759, 343, 779, 905, 108]\n",
        "        df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "\n",
        "        if 'datahoraenvio' in df.columns:\n",
        "            df['datahoraenvio'] = cudf.to_datetime(df['datahoraenvio'].astype('int64'), unit='ms')\n",
        "        if 'datahora' in df.columns:\n",
        "            df['datahora_converted'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "\n",
        "        if 'latitude' in df.columns:\n",
        "            df['latitude'] = df['latitude'].str.replace(',', '.').astype('float32')\n",
        "        if 'longitude' in df.columns:\n",
        "            df['longitude'] = df['longitude'].str.replace(',', '.').astype('float32')\n",
        "\n",
        "        '''\n",
        "        Faz a interpolação e preenche os valores nulos.\n",
        "        '''\n",
        "        while df['latitude'].isnull().any() or df['longitude'].isnull().any():\n",
        "            df['latitude'] = df['latitude'].interpolate().ffill().bfill()\n",
        "            df['longitude'] = df['longitude'].interpolate().ffill().bfill()\n",
        "\n",
        "        if 'velocidade' in df.columns:\n",
        "            df['velocidade'] = df['velocidade'].astype('int32')\n",
        "            df = df[(df['velocidade'] >= 0) & (df['velocidade'] <= 250)]\n",
        "        if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "            df = df[df['latitude'].between(-90, 90) & df['longitude'].between(-180, 180)]\n",
        "\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Faz o pré-processamento do DataFrame de teste, adicionando colunas com informações de data e \n",
        "    hora extraídas do nome do arquivo e convertendo a coluna `datahora`.\n",
        "    '''\n",
        "    def preprocess_test_df(self, df, json_path, date_from_file):\n",
        "        hour_from_file_test = int(json_path.split('_')[-1].split('.')[0])\n",
        "        df['hour_from_file_test'] = hour_from_file_test\n",
        "        df['date_hour_from_file_test'] = f\"{date_from_file}_{hour_from_file_test}\"\n",
        "        df['date_from_file'] = date_from_file\n",
        "\n",
        "        if 'datahora' in df.columns:\n",
        "            df['datahora_converted'] = cudf.to_datetime(df['datahora'].astype('int64'), unit='ms')\n",
        "\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Processa arquivos de teste encontrados no diretório intermediário e salva os DataFrames processados \n",
        "    como arquivos CSV no diretório de processados.\n",
        "    '''\n",
        "    def process_and_save_test_files(self, date_from_file):\n",
        "        '''\n",
        "        Lista de arquivos de teste no diretório intermediário.\n",
        "        '''\n",
        "        test_files = []\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith('teste') and file.endswith('.json'):\n",
        "                    test_files.append(os.path.join(root, file))\n",
        "\n",
        "        print(f\"Found {len(test_files)} test files to process for date: {date_from_file}\")\n",
        "\n",
        "        '''\n",
        "        Loop através de cada arquivo de teste encontrado.\n",
        "        '''\n",
        "        for test_file in tqdm(test_files, desc=f\"Processing test files for {date_from_file}\"):\n",
        "            print(f\"Processing file: {test_file}\")\n",
        "            df = self.process_json(test_file)\n",
        "            df = self.preprocess_test_df(df, test_file, date_from_file)\n",
        "            hour_from_file_test = int(test_file.split('_')[-1].split('.')[0])\n",
        "            output_csv = os.path.join(self.processados_teste_dir, f'test_data_{date_from_file}_{hour_from_file_test}.csv')\n",
        "            print(f\"Saving processed data to: {output_csv}\")\n",
        "            df.to_pandas().to_csv(output_csv, index=False)\n",
        "\n",
        "    '''\n",
        "    Processa e concatena arquivos de treino encontrados no diretório intermediário, \n",
        "    salvando os DataFrames concatenados como arquivos CSV no diretório de processados.\n",
        "    '''\n",
        "    def concatenate_and_save_train_files(self, date_from_file):\n",
        "        '''\n",
        "        Lista de arquivos de treino no diretório intermediário.\n",
        "        '''\n",
        "        train_files = []\n",
        "        for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "            for file in files:\n",
        "                if file.startswith('2024') and file.endswith('.json'):\n",
        "                    train_files.append(os.path.join(root, file))\n",
        "\n",
        "        train_files = sorted(train_files)\n",
        "        '''\n",
        "        Itera sobre os arquivos de treinos de 2 em 2. A razão disso é porque cada arquivo Json representa\n",
        "        1 hora. Como eu precisarei treinar 2 horas consecutivas, irei criar arquivos csv concatenando\n",
        "        2 jsons em um único csv. Por exemplo, o arquivo 2024-05-16_06.json e 2024-05-16_07.json, será \n",
        "        transformado em um único arquivo train_data_2024-05-16_06_07.csv\n",
        "        '''\n",
        "        for i in tqdm(range(0, len(train_files), 2), desc=f\"Processing train files for {date_from_file}\"):\n",
        "            if i + 1 < len(train_files):\n",
        "                df1 = self.process_json(train_files[i])\n",
        "                df2 = self.process_json(train_files[i + 1])\n",
        "                df1 = self.preprocess_train_df(df1, train_files[i], date_from_file)\n",
        "                df2 = self.preprocess_train_df(df2, train_files[i + 1], date_from_file)\n",
        "                concatenated_df = cudf.concat([df1, df2], ignore_index=True)\n",
        "                hour_from_file_train1 = int(train_files[i].split('_')[-1].split('.')[0])\n",
        "                hour_from_file_train2 = int(train_files[i + 1].split('_')[-1].split('.')[0])\n",
        "                output_csv = os.path.join(self.processados_teste_dir, f'train_data_{date_from_file}_{hour_from_file_train1}_{hour_from_file_train2}.csv')\n",
        "                print(f\"Saving processed data to: {output_csv}\")\n",
        "                concatenated_df.to_pandas().to_csv(output_csv, index=False)\n",
        "\n",
        "    '''\n",
        "    Inicia a execução de todas as funções\n",
        "    '''\n",
        "    def run(self):\n",
        "        zip_files = glob(os.path.join(self.base_dir, '*.zip'))\n",
        "        if not zip_files:\n",
        "            raise FileNotFoundError(\"No zip files found in the specified directory.\")\n",
        "\n",
        "        '''\n",
        "        Loop através de cada arquivo zip encontrado.\n",
        "        '''\n",
        "        for zip_file in zip_files:\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.intermediarios_teste_dir)\n",
        "                date_from_file = '-'.join(os.path.basename(zip_file).split('-')[1:4]).split('.')[0]\n",
        "                print(f\"DATE FROM FILE: {date_from_file}\")\n",
        "                self.process_json_files(date_from_file)\n",
        "                '''\n",
        "                Limpa o diretório intermediário para evitar misturas de arquivos de diferentes arquivos zip.\n",
        "                '''\n",
        "                for root, _, files in os.walk(self.intermediarios_teste_dir):\n",
        "                    for file in files:\n",
        "                        os.remove(os.path.join(root, file))\n",
        "\n",
        "'''\n",
        "Executa o código\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    processor = FinalDataProcessor(base_dir)\n",
        "    processor.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Pre processing the data</h3>\n",
        "<p>Agora eu estou aplicando alguns pre processamentos, como a filtragem por linhas ( por alguma razão, a filtragem anterior não funcionou), a diminuição do tamanho da palavra do tipo de dados para tornar o arquivo final menor, e a exclusão de outliers. Outliers são todos os pontos de latitude e longitude que fogem da rota usual da linha ao longo do dia.Outliers incluem garagens e pontos finais. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Lista de linhas válidas de acordo com o enunciado.\n",
        "'''\n",
        "linhas_validas = [\n",
        "    483, 864, 639, 3, 309, 774, 629, 371, 397, 100, 838, 315, 624, 388, 918, 665, 328, 497, 878, 355, 138, 606, 457, \n",
        "    550, 803, 917, 638, 2336, 399, 298, 867, 553, 565, 422, 756, 186012003, 292, 554, 634, 232, 415, 2803, 324, 852, \n",
        "    557, 759, 343, 779, 905, 108\n",
        "]\n",
        "\n",
        "'''\n",
        "Classe responsável pelo pré-processamento de dados, incluindo carregamento, conversão de tipos, \n",
        "filtragem de dados, remoção de outliers e salvamento de arquivos intermediários e finais.\n",
        "'''\n",
        "class DataPreprocessing:\n",
        "    def __init__(self, file_path=None, intermediarios_dir=None, processados_dir=None, output_filename=None, chunk_size=1000000):\n",
        "        self.file_path = file_path\n",
        "        self.intermediarios_dir = intermediarios_dir\n",
        "        self.processados_dir = processados_dir\n",
        "        self.cleaned_file = os.path.join(intermediarios_dir, f'cleaned_{output_filename}') if intermediarios_dir and output_filename else None\n",
        "        self.prepared_file = os.path.join(processados_dir, 'treated', output_filename) if processados_dir and output_filename else None\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def load_data(self):\n",
        "        if self.file_path.endswith('.csv'):\n",
        "            return cudf.read_csv(self.file_path)\n",
        "        elif self.file_path.endswith('.parquet'):\n",
        "            return cudf.read_parquet(self.file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def save_intermediate_data(self, df, filename):\n",
        "        df.to_parquet(filename)\n",
        "\n",
        "    '''\n",
        "    Conversão de dados necessária.\n",
        "    '''\n",
        "    def convert_dtypes(self, df):\n",
        "        if \"latitude\" in df.columns and (df[\"latitude\"].dtype == 'object' or df[\"latitude\"].dtype.name == 'category'):\n",
        "            df[\"latitude\"] = df[\"latitude\"].str.replace(',', '.').astype('float32')\n",
        "        if \"longitude\" in df.columns and (df[\"longitude\"].dtype == 'object' or df[\"longitude\"].dtype.name == 'category'):\n",
        "            df[\"longitude\"] = df[\"longitude\"].str.replace(',', '.').astype('float32')\n",
        "        if \"datahora\" in df.columns:\n",
        "            df['datahora'] = df['datahora'].astype('int64')\n",
        "        if \"linha\" in df.columns:\n",
        "            df['linha'] = df['linha'].astype('int32')\n",
        "        if \"ordem\" in df.columns:\n",
        "            df['ordem'] = df['ordem'].astype('object')\n",
        "        if \"velocidade\" in df.columns:\n",
        "            df['velocidade'] = df['velocidade'].astype('int32')\n",
        "        if \"datahoraservidor\" in df.columns:\n",
        "            df['datahoraservidor'] = df['datahoraservidor'].astype('int64')\n",
        "        if \"dia_da_semana\" in df.columns:\n",
        "            df['dia_da_semana'] = df['dia_da_semana'].astype('int32')\n",
        "        if \"hora\" in df.columns:\n",
        "            df['hora'] = df['hora'].astype('int16')\n",
        "        if \"diff_timestamp\" in df.columns:\n",
        "            df['diff_timestamp'] = df['diff_timestamp'].astype('float64')\n",
        "        if \"latitude_diff\" in df.columns:\n",
        "            df['latitude_diff'] = df['latitude_diff'].astype('float32')\n",
        "        if \"longitude_diff\" in df.columns:\n",
        "            df['longitude_diff'] = df['longitude_diff'].astype('float32')\n",
        "        if \"distancia\" in df.columns:\n",
        "            df['distancia'] = df['distancia'].astype('float32')\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Filtra o DataFrame mantendo apenas as linhas válidas de acordo com o enunciado.\n",
        "    '''\n",
        "    def filter_valid_linhas(self, df):\n",
        "        if \"linha\" in df.columns:\n",
        "            df = df[df['linha'].astype(str).isin(map(str, linhas_validas))]\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    Remove outliers dos dados usando o algoritmo DBSCAN. \n",
        "    Para identificar outliers, eu verifiquei a rota que uma mesma linha faz ao longo do tempo.\n",
        "    Ou seja, eu verifiquei sua latitude e longitude ao longo do tempo. Tudo que foge dessa rota,\n",
        "    eu considerei como outliers. Isso inclui garagens de pontos finais.\n",
        "    '''\n",
        "    def remove_outliers(self, df, eps=0.001, min_samples=10):\n",
        "        if os.path.exists(self.cleaned_file):\n",
        "            df = cudf.read_parquet(self.cleaned_file)\n",
        "            if 'datahora' in df.columns:\n",
        "                return df\n",
        "\n",
        "        if 'datahora_converted' not in df.columns and 'datahora' in df.columns:\n",
        "            df['datahora_converted'] = cudf.to_datetime(df['datahora'], unit='ms')\n",
        "\n",
        "        if 'linha' not in df.columns or 'ordem' not in df.columns:\n",
        "            print(df.head(2))\n",
        "            raise KeyError(\"Necessary columns are not present in the data.\")\n",
        "\n",
        "        if 'latitude' not in df.columns or 'longitude' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        '''\n",
        "        Obtém as linhas únicas e converte para pandas.\n",
        "        '''\n",
        "        lines = df['linha'].unique().to_pandas()\n",
        "        cleaned_dfs = []\n",
        "\n",
        "        '''\n",
        "        Loop através de cada linha única.\n",
        "        '''\n",
        "        for linha in tqdm(lines, desc=\"Lines\", leave=False, dynamic_ncols=True):\n",
        "            line_data = df[df['linha'] == linha].copy()\n",
        "            if len(line_data) < min_samples:\n",
        "                cleaned_dfs.append(line_data)\n",
        "                continue\n",
        "\n",
        "            if 'latitude' in line_data.columns and 'longitude' in line_data.columns:\n",
        "                other_columns = [col for col in line_data.columns if col not in ['latitude', 'longitude']]\n",
        "                line_data_pd = line_data[['latitude', 'longitude']].to_pandas()\n",
        "                db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = db.fit_predict(line_data_pd)\n",
        "\n",
        "                line_data['labels'] = labels\n",
        "                non_outliers = line_data[line_data['labels'] != -1].copy()\n",
        "                non_outliers = non_outliers.drop(columns=['labels'])\n",
        "\n",
        "                '''\n",
        "                Mescla os dados não outliers com outras colunas.\n",
        "                '''\n",
        "                for col in other_columns:\n",
        "                    non_outliers[col] = line_data[col].loc[non_outliers.index]\n",
        "\n",
        "                cleaned_dfs.append(non_outliers)\n",
        "\n",
        "        '''\n",
        "        Concatena todos os DataFrames limpos e salva os dados intermediários.\n",
        "        '''\n",
        "        if cleaned_dfs:\n",
        "            cleaned_df = cudf.concat(cleaned_dfs, ignore_index=True)\n",
        "            self.save_intermediate_data(cleaned_df, self.cleaned_file)\n",
        "        else:\n",
        "            raise ValueError(\"No objects to concatenate\")\n",
        "\n",
        "        return cleaned_df\n",
        "\n",
        "    '''\n",
        "    Realiza o pré-processamento completo dos dados, incluindo carregamento, conversão de tipos, \n",
        "    filtragem de linhas válidas, remoção de outliers e salvamento dos dados processados.\n",
        "    '''\n",
        "    def preprocess(self, remove_outliers=False):\n",
        "        print(f\"Processing file: {self.file_path}\")\n",
        "        df = self.load_data()\n",
        "        df = self.convert_dtypes(df)\n",
        "        df = self.filter_valid_linhas(df)\n",
        "        if remove_outliers and 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "            df = self.remove_outliers(df)\n",
        "        os.makedirs(os.path.dirname(self.prepared_file), exist_ok=True)\n",
        "        df.to_parquet(self.prepared_file)\n",
        "        df.to_csv(self.prepared_file.replace('.parquet', '.csv'), index=False)\n",
        "        print(f\"Processed and saved file: {self.prepared_file}\")\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting script execution...\")\n",
        "    base_dir = os.getcwd()\n",
        "    print(f\"Base directory: {base_dir}\")\n",
        "\n",
        "    #Gera os paths necessários\n",
        "    base_final_dir = os.path.join(base_dir, 'dados_finais')\n",
        "    intermediarios_teste_dir = os.path.join(base_final_dir, 'intermediarios_teste')\n",
        "    processados_teste_dir = os.path.join(base_final_dir, 'processados_teste')\n",
        "    print(f\"Base final directory: {base_final_dir}\")\n",
        "    print(f\"Intermediarios teste directory: {intermediarios_teste_dir}\")\n",
        "    print(f\"Processados teste directory: {processados_teste_dir}\")\n",
        "\n",
        "    os.makedirs(os.path.join(processados_teste_dir, 'treated'), exist_ok=True)\n",
        "\n",
        "    #Processa os arquivos de teste e treino separadamente\n",
        "    for root, _, files in os.walk(processados_teste_dir):\n",
        "        print(f\"Current directory: {root}\")\n",
        "        print(f\"Files: {files}\")\n",
        "        for file in tqdm(files, desc=\"Processing files\", leave=False, dynamic_ncols=True):\n",
        "            print(f\"Processing file: {file}\")\n",
        "            if file.startswith('train_data_') and file.endswith('.csv'):\n",
        "                csv_file_path = os.path.join(root, file)\n",
        "                parquet_file_path = csv_file_path.replace('.csv', '.parquet')\n",
        "                print(f\"Converting {csv_file_path} to {parquet_file_path}\")\n",
        "\n",
        "                #Converte CSV para Parquet\n",
        "                df = pd.read_csv(csv_file_path)\n",
        "                df.to_parquet(parquet_file_path)\n",
        "\n",
        "                #Pré processamento feito aqui\n",
        "                treino_preprocessing = DataPreprocessing(\n",
        "                    file_path=parquet_file_path,\n",
        "                    intermediarios_dir=intermediarios_teste_dir,\n",
        "                    processados_dir=processados_teste_dir,\n",
        "                    output_filename=file.replace('.csv', '.parquet')\n",
        "                )\n",
        "                treino_preprocessing.preprocess(remove_outliers=True)\n",
        "\n",
        "            if file.startswith('test_data_') and file.endswith('.csv'):\n",
        "                csv_file_path = os.path.join(root, file)\n",
        "                parquet_file_path = csv_file_path.replace('.csv', '.parquet')\n",
        "                print(f\"Converting {csv_file_path} to {parquet_file_path}\")\n",
        "\n",
        "                #Converte CSV para Parquet\n",
        "                df = pd.read_csv(csv_file_path)\n",
        "                df.to_parquet(parquet_file_path)\n",
        "\n",
        "                #Pré processamento para as features de  latitude and longitude\n",
        "                if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "                    lat_long_preprocessing = DataPreprocessing(\n",
        "                        file_path=parquet_file_path,\n",
        "                        intermediarios_dir=intermediarios_teste_dir,\n",
        "                        processados_dir=processados_teste_dir,\n",
        "                        output_filename=file.replace('test_data_', 'treated_test_lat_long_').replace('.csv', '.parquet')\n",
        "                    )\n",
        "                    lat_long_preprocessing.preprocess(remove_outliers=False)\n",
        "\n",
        "                #Pré processamento para as features de datahora\n",
        "                if 'datahora' in df.columns:\n",
        "                    datahora_preprocessing = DataPreprocessing(\n",
        "                        file_path=parquet_file_path,\n",
        "                        intermediarios_dir=intermediarios_teste_dir,\n",
        "                        processados_dir=processados_teste_dir,\n",
        "                        output_filename=file.replace('test_data_', 'treated_test_datahora_').replace('.csv', '.parquet')\n",
        "                    )\n",
        "                    datahora_preprocessing.preprocess(remove_outliers=False)\n",
        "\n",
        "    print(\"Script execution finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Processing the data and creating the features</h3>\n",
        "<p>A estratégia abaixo divide os arquivos de treino em df_datahora e df_lat_long. O objetivo é treinar arquivos separados para cada previsão que precisa ser feita. Além do mais, encoder específico para a coluna ordem foi criada, pois eu preciso transformar essa coluna em número mas sem alterar muito os seus valores. Também foram criadas novas features importantes, especialmente a velocidade média tanto para os datasets de treino quanto para os datasets de teste</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Usar a função convert_dtypes diretamente da classe Preprocessing\n",
        "def convert_dtypes(df):\n",
        "    if \"latitude\" in df.columns and (df[\"latitude\"].dtype == 'object' or df[\"latitude\"].dtype.name == 'category'):\n",
        "        df[\"latitude\"] = df[\"latitude\"].str.replace(',', '.').astype('float32')\n",
        "    if \"longitude\" in df.columns and (df[\"longitude\"].dtype == 'object' or df[\"longitude\"].dtype.name == 'category'):\n",
        "        df[\"longitude\"] = df[\"longitude\"].str.replace(',', '.').astype('float32')\n",
        "    if \"datahora\" in df.columns:\n",
        "        df['datahora'] = df['datahora'].astype('int64')\n",
        "    if \"linha\" in df.columns:\n",
        "        df['linha'] = df['linha'].astype('int32')\n",
        "    if \"ordem\" in df.columns:\n",
        "        df['ordem'] = df['ordem'].astype('object')\n",
        "    if \"velocidade\" in df.columns:\n",
        "        df['velocidade'] = df['velocidade'].astype('int32')\n",
        "    if \"datahoraservidor\" in df.columns:\n",
        "        df['datahoraservidor'] = df['datahoraservidor'].astype('int64')\n",
        "    if \"day_of_week\" in df.columns:\n",
        "        df['day_of_week'] = df['day_of_week'].astype('int32')\n",
        "    if \"hour\" in df.columns:\n",
        "        df['hour'] = df['hour'].astype('int16')\n",
        "    if \"minute\" in df.columns:\n",
        "        df['minute'] = df['minute'].astype('int16')\n",
        "    if \"diff_timestamp\" in df.columns:\n",
        "        df['diff_timestamp'] = df['diff_timestamp'].astype('float64')\n",
        "    if \"latitude_diff\" in df.columns:\n",
        "        df['latitude_diff'] = df['latitude_diff'].astype('float32')\n",
        "    if \"longitude_diff\" in df.columns:\n",
        "        df['longitude_diff'] = df['longitude_diff'].astype('float32')\n",
        "    if \"distancia\" in df.columns:\n",
        "        df['distancia'] = df['distancia'].astype('float32')\n",
        "    return df\n",
        "\n",
        "'''\n",
        "Remove colunas desnecessárias e divide o DataFrame em dois: um para dados de data e hora, e outro para \n",
        "dados de latitude e longitude.\n",
        "'''\n",
        "def preprocess_and_split(df):\n",
        "    columns_to_drop = ['datahoraservidor', 'datahoraenvio', 'date_hour_from_file_test']\n",
        "    df_datahora = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    df_lat_long = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    return df_datahora, df_lat_long\n",
        "\n",
        "'''\n",
        "Codifica a coluna `ordem`, transformando valores que começam com uma letra em um número correspondente \n",
        "e mantendo os demais valores como inteiros.\n",
        "'''\n",
        "def custom_encode_ordem(df, column='ordem'):\n",
        "    def encode_value(val):\n",
        "        if isinstance(val, str) and val[0].isalpha():\n",
        "            letter = val[0].upper()\n",
        "            number = ord(letter) - ord('A') + 1\n",
        "            encoded_val = str(number) + val[1:]\n",
        "            return int(encoded_val)\n",
        "        elif str(val).isdigit():\n",
        "            return int(val)\n",
        "        return val\n",
        "\n",
        "    df[column] = df[column].apply(encode_value).astype('int32')\n",
        "    return df\n",
        "\n",
        "'''\n",
        "Cria novas features no DataFrame com base em colunas existentes, como `datahora`, `latitude` e `longitude`.\n",
        "'''\n",
        "def apply_feature_engineering(df, target):\n",
        "    try:\n",
        "        print(f\"Creating new features for {target}...\")\n",
        "        if 'datahora' in df.columns:\n",
        "            df['datahora_converted'] = pd.to_datetime(df['datahora'], unit='ms')\n",
        "\n",
        "        if 'datahora_converted' in df.columns and 'linha' in df.columns and 'ordem' in df.columns:\n",
        "            df = df.sort_values(by=['linha', 'ordem', 'datahora_converted'])\n",
        "            df['day_of_week'] = df['datahora_converted'].dt.weekday\n",
        "            df['hour'] = df['datahora_converted'].dt.hour\n",
        "            df['minute'] = df['datahora_converted'].dt.minute\n",
        "            df['diff_timestamp'] = df.groupby(['linha', 'ordem'])['datahora_converted'].diff().astype('int64') / 10**6\n",
        "\n",
        "        if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "            df['latitude_diff'] = df.groupby(['linha', 'ordem'])['latitude'].diff().ffill().bfill()\n",
        "            df['longitude_diff'] = df.groupby(['linha', 'ordem'])['longitude'].diff().ffill().bfill()\n",
        "\n",
        "            lat1 = np.radians(df['latitude'])\n",
        "            lon1 = np.radians(df['longitude'])\n",
        "            lat2 = np.radians(df['latitude'] + df['latitude_diff'])\n",
        "            lon2 = np.radians(df['longitude'] + df['longitude_diff'])\n",
        "\n",
        "            dlat = lat2 - lat1\n",
        "            dlon = lon2 - lon1\n",
        "\n",
        "            a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "            c = 2 * np.arcsin(np.sqrt(a))\n",
        "            r = 6371\n",
        "            df['distancia'] = c * r * 1000\n",
        "\n",
        "        print(f\"New features created for {target}: 'day_of_week', 'hour', 'minute', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in creating new features: {e}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        print(f\"Encoding categorical features for {target}...\")\n",
        "        df = custom_encode_ordem(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in encoding categorical features: {e}\")\n",
        "        raise\n",
        "\n",
        "    return df\n",
        "\n",
        "'''\n",
        "Adiciona a velocidade média ao DataFrame de treino e teste, com base em colunas como `datahora`, \n",
        "`latitude` e `longitude`.\n",
        "'''\n",
        "def add_avg_velocity(train_df, test_df, target):\n",
        "    train_df = convert_dtypes(train_df.copy())\n",
        "    test_df = convert_dtypes(test_df.copy())\n",
        "\n",
        "    train_df = custom_encode_ordem(train_df)\n",
        "    test_df = custom_encode_ordem(test_df)\n",
        "\n",
        "    '''\n",
        "    Como os datasets de teste não possuem a columa velocidade, precisei criar alguns mecanismos para \n",
        "    que eu pudesse adicionar a coluna de velocidade média nos datasets de teste. Adicionei a coluna\n",
        "    velocidade média no dataset de treino agroupando por linha e ordem. Tentei agrupar por algumas \n",
        "    features de datahora também, mas estava dando erro. Eu extraio esses valores agrupados para uma \n",
        "    variável chamada avg_speed e então eu faço merge nos datasets de teste. \n",
        "    '''\n",
        "    if target == 'datahora':\n",
        "        train_df['datahora_converted'] = pd.to_datetime(train_df['datahora'], unit='ms')\n",
        "        test_df['datahora_converted'] = pd.to_datetime(test_df['datahora'], unit='ms')\n",
        "        if 'datahora_converted' in train_df.columns:\n",
        "            avg_speed = train_df.groupby(['linha', 'ordem']).agg({'velocidade': 'mean'}).reset_index()\n",
        "            avg_speed.columns = ['linha', 'ordem', 'velocidade_for_datahora']\n",
        "\n",
        "            train_df = train_df.merge(avg_speed, on=['linha', 'ordem'], how='left')\n",
        "            train_df['velocidade_for_datahora'] = train_df['velocidade_for_datahora'].ffill().bfill()\n",
        "\n",
        "        if 'datahora_converted' in test_df.columns:\n",
        "            test_df = test_df.merge(avg_speed, on=['linha', 'ordem'], how='left')\n",
        "            print(\"Test DataFrame after merge:\")\n",
        "            print(test_df.head())\n",
        "            test_df['velocidade_for_datahora'] = test_df['velocidade_for_datahora'].ffill().bfill()\n",
        "            test_df = test_df.rename(columns={'velocidade_for_datahora': 'avg_speed'})\n",
        "            train_df = train_df.rename(columns={'velocidade_for_datahora': 'avg_speed'})\n",
        "            \n",
        "    elif target == 'lat_long':\n",
        "        if 'latitude' in train_df.columns and 'longitude' in train_df.columns:\n",
        "            train_df['lat_temp'] = (train_df['latitude'] * 100).astype(int)\n",
        "            train_df['lon_temp'] = (train_df['longitude'] * 100).astype(int)\n",
        "\n",
        "            avg_speed = train_df.groupby(['linha', 'ordem', 'lat_temp', 'lon_temp']).agg({'velocidade': 'mean'}).reset_index()\n",
        "            avg_speed.columns = ['linha', 'ordem', 'lat_temp', 'lon_temp', 'velocidade_for_lat_long']\n",
        "\n",
        "            train_df = train_df.merge(avg_speed, on=['linha', 'ordem', 'lat_temp', 'lon_temp'], how='left')\n",
        "            train_df['velocidade_for_lat_long'] = train_df['velocidade_for_lat_long'].ffill().bfill()\n",
        "\n",
        "        if 'latitude' in test_df.columns and 'longitude' in test_df.columns:\n",
        "            test_df['lat_temp'] = (test_df['latitude'] * 100).astype(int)\n",
        "            test_df['lon_temp'] = (test_df['longitude'] * 100).astype(int)\n",
        "\n",
        "            test_df = test_df.merge(avg_speed, on=['linha', 'ordem', 'lat_temp', 'lon_temp'], how='left')\n",
        "            test_df['velocidade_for_lat_long'] = test_df['velocidade_for_lat_long'].ffill().bfill()\n",
        "\n",
        "            test_df = test_df.drop(columns=['lat_temp', 'lon_temp'])\n",
        "            train_df = train_df.drop(columns=['lat_temp', 'lon_temp'])\n",
        "\n",
        "            test_df = test_df.rename(columns={'velocidade_for_lat_long': 'avg_speed'})\n",
        "            train_df = train_df.rename(columns={'velocidade_for_lat_long': 'avg_speed'})\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "'''\n",
        "Ordena as colunas do DataFrame, mantendo colunas iniciais e finais especificadas em suas respectivas \n",
        "posições.\n",
        "'''\n",
        "def sort_columns(df, initial_columns=None, final_columns=None):\n",
        "    if initial_columns is None:\n",
        "        initial_columns = []\n",
        "    if final_columns is None:\n",
        "        final_columns = []\n",
        "\n",
        "    all_columns = list(df.columns)\n",
        "    columns_to_sort = [col for col in all_columns if col not in initial_columns + final_columns]\n",
        "    sorted_columns = sorted(columns_to_sort)\n",
        "    new_order = initial_columns + sorted_columns + final_columns\n",
        "\n",
        "    df = df[new_order]\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = os.getcwd()\n",
        "    base_final_dir = os.path.join(base_dir, 'dados_finais')\n",
        "    processados_teste_dir = os.path.join(base_final_dir, 'processados_teste', 'treated')\n",
        "    feature_modeling_dir = os.path.join(base_final_dir, 'feature_modeling')\n",
        "\n",
        "    os.makedirs(feature_modeling_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        print(\"Loading datasets...\")\n",
        "        train_files = [f for f in os.listdir(processados_teste_dir) if f.startswith('train_data_') and f.endswith('.csv')]\n",
        "        test_datahora_files = [f for f in os.listdir(processados_teste_dir) if f.startswith('treated_test_datahora_') and f.endswith('.csv')]\n",
        "        test_lat_long_files = [f for f in os.listdir(processados_teste_dir) if f.startswith('treated_test_lat_long_') and f.endswith('.csv')]\n",
        "\n",
        "        '''\n",
        "        Processa cada arquivo de treino, aplicando pré-processamento e engenharia de features.\n",
        "        '''\n",
        "        for train_file in tqdm(train_files, desc=\"Processing training files\"):\n",
        "            try:\n",
        "                print(f\"Processing file: {train_file}\")\n",
        "                train_df = pd.read_csv(os.path.join(processados_teste_dir, train_file))\n",
        "\n",
        "                train_datahora_df, train_lat_long_df = preprocess_and_split(train_df)\n",
        "\n",
        "                print(\"Adding average velocity to training data...\")\n",
        "                train_datahora_df, _ = add_avg_velocity(train_datahora_df, train_datahora_df.copy(), 'datahora')\n",
        "                train_lat_long_df, _ = add_avg_velocity(train_lat_long_df, train_lat_long_df.copy(), 'lat_long')\n",
        "\n",
        "                train_datahora_df = apply_feature_engineering(train_datahora_df, 'datahora')\n",
        "                train_lat_long_df = apply_feature_engineering(train_lat_long_df, 'lat_long')\n",
        "\n",
        "                print(\"Dropping rows with null values in train_datahora dataframe...\")\n",
        "                print(f\"Shape before dropna: {train_datahora_df.shape}\")\n",
        "                train_datahora_df = train_datahora_df.dropna(subset=['diff_timestamp', 'avg_speed'])\n",
        "                print(f\"Shape after dropna: {train_datahora_df.shape}\")\n",
        "                if train_datahora_df.empty:\n",
        "                    raise ValueError(f\"DataFrame train_datahora for file {train_file} is empty after dropping null values.\")\n",
        "\n",
        "                print(\"Dropping rows with null values in train_lat_long dataframe...\")\n",
        "                print(f\"Shape before dropna: {train_lat_long_df.shape}\")\n",
        "                train_lat_long_df = train_lat_long_df.dropna(subset=['latitude_diff', 'longitude_diff'])\n",
        "                print(f\"Shape after dropna: {train_lat_long_df.shape}\")\n",
        "                if train_lat_long_df.empty:\n",
        "                    raise ValueError(f\"DataFrame train_lat_long for file {train_file} is empty after dropping null values.\")\n",
        "\n",
        "                train_datahora_df.to_csv(os.path.join(feature_modeling_dir, f'preprocessed_{train_file.replace(\".csv\", \"_datahora.csv\")}'), index=False)\n",
        "                train_lat_long_df.to_csv(os.path.join(feature_modeling_dir, f'preprocessed_{train_file.replace(\".csv\", \"_lat_long.csv\")}'), index=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {train_file}: {e}\")\n",
        "\n",
        "        '''\n",
        "        Processa cada arquivo de teste `datahora`, aplicando pré-processamento e engenharia de features.\n",
        "        '''\n",
        "        for test_file in tqdm(test_datahora_files, desc=\"Processing test datahora files\"):\n",
        "            try:\n",
        "                print(f\"Processing test datahora file: {test_file}\")\n",
        "                test_datahora_df = pd.read_csv(os.path.join(processados_teste_dir, test_file))\n",
        "\n",
        "                train_datahora_df = pd.read_csv(os.path.join(processados_teste_dir, train_files[0]))  # Read the first train file for adding avg_velocity\n",
        "\n",
        "                _, test_datahora_df = add_avg_velocity(train_datahora_df, test_datahora_df, 'datahora')\n",
        "                test_datahora_df = apply_feature_engineering(test_datahora_df, 'datahora')\n",
        "\n",
        "                print(\"Dropping rows with null values in test_datahora dataframe...\")\n",
        "                print(f\"Shape before dropna: {test_datahora_df.shape}\")\n",
        "                test_datahora_df = test_datahora_df.dropna(subset=['diff_timestamp', 'avg_speed'])\n",
        "                print(f\"Shape after dropna: {test_datahora_df.shape}\")\n",
        "                if test_datahora_df.empty:\n",
        "                    raise ValueError(f\"DataFrame test_datahora for file {test_file} is empty after dropping null values.\")\n",
        "\n",
        "                test_datahora_df.to_csv(os.path.join(feature_modeling_dir, f'preprocessed_{test_file}'), index=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {test_file}: {e}\")\n",
        "\n",
        "        '''\n",
        "        Processa cada arquivo de teste `lat_long`, aplicando pré-processamento e engenharia de features.\n",
        "        '''\n",
        "        for test_file in tqdm(test_lat_long_files, desc=\"Processing test lat_long files\"):\n",
        "            try:\n",
        "                print(f\"Processing test lat_long file: {test_file}\")\n",
        "                test_lat_long_df = pd.read_csv(os.path.join(processados_teste_dir, test_file))\n",
        "\n",
        "                train_lat_long_df = pd.read_csv(os.path.join(processados_teste_dir, train_files[0]))  # Read the first train file for adding avg_velocity\n",
        "\n",
        "                _, test_lat_long_df = add_avg_velocity(train_lat_long_df, test_lat_long_df, 'lat_long')\n",
        "                test_lat_long_df = apply_feature_engineering(test_lat_long_df, 'lat_long')\n",
        "\n",
        "                print(\"Dropping rows with null values in test_lat_long dataframe...\")\n",
        "                print(f\"Shape before dropna: {test_lat_long_df.shape}\")\n",
        "                test_lat_long_df = test_lat_long_df.dropna(subset=['latitude_diff', 'longitude_diff'])\n",
        "                print(f\"Shape after dropna: {test_lat_long_df.shape}\")\n",
        "                if test_lat_long_df.empty:\n",
        "                    raise ValueError(f\"DataFrame test_lat_long for file {test_file} is empty after dropping null values.\")\n",
        "\n",
        "                test_lat_long_df.to_csv(os.path.join(feature_modeling_dir, f'preprocessed_{test_file}'), index=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {test_file}: {e}\")\n",
        "\n",
        "        print(\"Feature engineering and preprocessing complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in the main processing block: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Fazendo Cross Validation</h3>\n",
        "<p>Aqui irei fazer a validação cruzada. Basicamente, teremos duas previsões a serem feitas de maneira separada. Para isso, irei fazer duas validações cruzadas, testando Random Forest,XGBoost e CatBoost com vários parâmetros. Para cada validação cruzada, o objetivo é encontrar o melhor modelo, com os melhores parâmetros. </p>\n",
        "<p>No caso da previsão de datahora, a métrica utilizada para a avaliação do melhor modelo é o RMSE. No caso da previsão de latitude, a métrica utilizada para a avaliação do melhor modelo é a média da distância Harversine</p>\n",
        "<p>Estou usando o Optuna para a validação cruzada e estou usando a aceleração por CUDA. </p>\n",
        "<p>Após isso, o melhor modelo final é escolhido para cada um das previsões feitas</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Define os caminhos para os diretórios onde serão armazenados os modelos intermediários e finais.\n",
        "'''\n",
        "base_path = 'dados_finais/feature_modeling'\n",
        "intermediate_model_path = 'dados_finais/cross_validation/intermediate_model'\n",
        "final_model_path = 'dados_finais/cross_validation/final_model'\n",
        "\n",
        "'''\n",
        "Garante que os diretórios existem, criando-os se necessário.\n",
        "'''\n",
        "os.makedirs(intermediate_model_path, exist_ok=True)\n",
        "os.makedirs(final_model_path, exist_ok=True)\n",
        "\n",
        "'''\n",
        "Função `train_and_evaluate` treina e avalia um modelo com os parâmetros especificados no `trial`. Retorna o modelo treinado e o RMSE.\n",
        "'''\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, model_name, trial, target_dimension):\n",
        "    try:\n",
        "        print(f\"Training model: {model_name} with trial parameters: {trial.params}\")\n",
        "        '''\n",
        "        Verifica qual modelo será treinado (CatBoost, XGBoost ou cuml_rf) e define os parâmetros específicos para cada um.\n",
        "        '''\n",
        "        if model_name == \"CatBoost\":\n",
        "            model = MultiOutputRegressor(\n",
        "                CatBoostRegressor(\n",
        "                    iterations=trial.suggest_int(\"iterations\", 100, 1000),\n",
        "                    learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
        "                    depth=trial.suggest_int(\"depth\", 4, 10),\n",
        "                    silent=True,\n",
        "                    task_type=\"GPU\" \n",
        "                )\n",
        "            )\n",
        "        elif model_name == \"XGBoost\":\n",
        "            model = MultiOutputRegressor(\n",
        "                XGBRegressor(\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "                    learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
        "                    max_depth=trial.suggest_int(\"max_depth\", 4, 10),\n",
        "                    tree_method='hist',\n",
        "                    device='cuda'\n",
        "                )\n",
        "            )\n",
        "        elif model_name == \"cuml_rf\":\n",
        "            if target_dimension == 1:\n",
        "                model = cuml_rf(\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "                    max_depth=trial.suggest_int(\"max_depth\", 4, 10)\n",
        "                )\n",
        "            else:\n",
        "                model = MultiOutputRegressor(\n",
        "                    cuml_rf(\n",
        "                        n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "                        max_depth=trial.suggest_int(\"max_depth\", 4, 10)\n",
        "                    )\n",
        "                )\n",
        "        '''\n",
        "        Treina o modelo e calcula o RMSE das previsões.\n",
        "        '''\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "        print(f\"Model {model_name} RMSE: {rmse}\")\n",
        "        return model, rmse\n",
        "    except Exception as e:\n",
        "        print(f\"Error in train_and_evaluate for model {model_name}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "'''\n",
        "Função `cross_validate_and_save` realiza a validação cruzada e salva o melhor modelo treinado. \n",
        "Recebe o caminho do arquivo de dados, as colunas alvo e o tipo de previsão como parâmetros.\n",
        "'''\n",
        "def cross_validate_and_save(file_path, target_cols, prediction_type):\n",
        "    try:\n",
        "        print(f\"Starting cross-validation for file: {file_path}\")\n",
        "        df = cudf.read_csv(file_path)\n",
        "        \n",
        "        '''\n",
        "        Verifica se o DataFrame foi carregado corretamente e possui as colunas esperadas.\n",
        "        '''\n",
        "        if df.empty:\n",
        "            print(f\"Dataframe loaded from {file_path} is empty.\")\n",
        "            return\n",
        "        if not all(col in df.columns for col in ['hour_from_file_train', 'date_from_file']):\n",
        "            print(f\"Missing expected columns in dataframe loaded from {file_path}. Columns found: {df.columns}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Columns in dataframe: {df.columns}\")\n",
        "        \n",
        "        file_info = {\n",
        "            'hour_from_file_train': df['hour_from_file_train'].unique().to_arrow().to_pylist(),\n",
        "            'date_from_file': df['date_from_file'].unique().to_arrow().to_pylist()\n",
        "        }\n",
        "        \n",
        "        '''\n",
        "        Define as colunas a serem descartadas com base no tipo de previsão.\n",
        "        '''\n",
        "        if prediction_type == 'lat_long':\n",
        "            drop_cols = ['datahora_converted', 'velocidade', 'latitude_diff', 'longitude_diff', 'distancia', 'day_of_week', 'hour', 'minute']\n",
        "        else:\n",
        "            drop_cols = ['velocidade', 'datahora_converted', 'day_of_week', 'hour', 'minute', 'diff_timestamp', 'latitude_diff', 'longitude_diff', 'distancia']\n",
        "        \n",
        "        print(f\"Dropping columns: {drop_cols}\")\n",
        "        df = df.drop(columns=drop_cols)\n",
        "        \n",
        "        hour_from_file_train_1, hour_from_file_train_2 = file_info['hour_from_file_train']\n",
        "        date_from_file = file_info['date_from_file'][0]\n",
        "\n",
        "        final_model_file = os.path.join(final_model_path, f\"final_model_predict_{prediction_type}_{date_from_file}_{hour_from_file_train_1}_{hour_from_file_train_2}.pkl\")\n",
        "        \n",
        "        '''\n",
        "        Verifica se o modelo final já existe para evitar a reprocessamento.\n",
        "        '''\n",
        "        if os.path.exists(final_model_file):\n",
        "            print(f\"Final model for {file_path} already exists. Skipping cross-validation.\")\n",
        "            return\n",
        "\n",
        "        train_data = df[df['hour_from_file_train'] == hour_from_file_train_1]\n",
        "        test_data = df[df['hour_from_file_train'] == hour_from_file_train_2]\n",
        "\n",
        "        if train_data.empty or test_data.empty:\n",
        "            print(f\"Train or test data is empty for {file_path}.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
        "\n",
        "        '''\n",
        "        Separa os dados de treino e teste em features (`X_train`, `X_test`) e targets (`y_train`, `y_test`).\n",
        "        '''\n",
        "        X_train = train_data.drop(columns=target_cols + ['hour_from_file_train', 'date_from_file', 'date_hour_from_file_train'])\n",
        "        y_train = train_data[target_cols]\n",
        "        X_test = test_data.drop(columns=target_cols + ['hour_from_file_train', 'date_from_file', 'date_hour_from_file_train'])\n",
        "        y_test = test_data[target_cols]\n",
        "\n",
        "        feature_names = X_train.columns.to_list()\n",
        "\n",
        "        X_train_np = X_train.to_numpy()\n",
        "        y_train_np = y_train.to_numpy()\n",
        "        X_test_np = X_test.to_numpy()\n",
        "        y_test_np = y_test.to_numpy()\n",
        "\n",
        "        best_model = None\n",
        "        best_rmse = float('inf')\n",
        "        best_params = None\n",
        "        best_model_name = None\n",
        "\n",
        "        '''\n",
        "        Loop que percorre os modelos a serem avaliados (CatBoost, XGBoost, cuml_rf).\n",
        "        '''\n",
        "        for model_name in tqdm([\"CatBoost\", \"XGBoost\", \"cuml_rf\"], desc=\"Models\", leave=False):\n",
        "            '''\n",
        "            Define a função objetivo para a otimização com o Optuna.\n",
        "            '''\n",
        "            def objective(trial):\n",
        "                model, rmse = train_and_evaluate(X_train_np, X_test_np, y_train_np, y_test_np, model_name, trial, len(target_cols))\n",
        "                return rmse\n",
        "\n",
        "            study = optuna.create_study(direction=\"minimize\")\n",
        "            study.optimize(objective, n_trials=10)\n",
        "            best_trial = study.best_trial\n",
        "\n",
        "            model, rmse = train_and_evaluate(X_train_np, X_test_np, y_train_np, y_test_np, model_name, best_trial, len(target_cols))\n",
        "\n",
        "            '''\n",
        "            Atualiza o melhor modelo se o RMSE atual for menor que o melhor RMSE encontrado até então.\n",
        "            '''\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_model = model\n",
        "                best_params = best_trial.params\n",
        "                best_model_name = model_name\n",
        "\n",
        "            intermediate_model_file = os.path.join(intermediate_model_path, f\"{model_name}_{prediction_type}_{date_from_file}_{hour_from_file_train_1}_{hour_from_file_train_2}.pkl\")\n",
        "            joblib.dump(model, intermediate_model_file)\n",
        "            print(f\"Saved intermediate model for {model_name} at {intermediate_model_file}\")\n",
        "\n",
        "        '''\n",
        "        Re-treina o melhor modelo com os dados de treino completos.\n",
        "        '''\n",
        "        print(f\"Re-training best model {best_model_name} on entire training data\")\n",
        "\n",
        "        if best_model_name == \"CatBoost\":\n",
        "            final_model = MultiOutputRegressor(\n",
        "                CatBoostRegressor(\n",
        "                    iterations=best_params[\"iterations\"],\n",
        "                    learning_rate=best_params[\"learning_rate\"],\n",
        "                    depth=best_params[\"depth\"],\n",
        "                    silent=True,\n",
        "                    task_type=\"GPU\"\n",
        "                )\n",
        "            )\n",
        "        elif best_model_name == \"XGBoost\":\n",
        "            final_model = MultiOutputRegressor(\n",
        "                XGBRegressor(\n",
        "                    n_estimators=best_params[\"n_estimators\"],\n",
        "                    learning_rate=best_params[\"learning_rate\"],\n",
        "                    max_depth=best_params[\"max_depth\"],\n",
        "                    tree_method='hist',\n",
        "                    device='cuda'\n",
        "                )\n",
        "            )\n",
        "        elif best_model_name == \"cuml_rf\":\n",
        "            if len(target_cols) == 1:\n",
        "                final_model = cuml_rf(\n",
        "                    n_estimators=best_params[\"n_estimators\"],\n",
        "                    max_depth=best_params[\"max_depth\"]\n",
        "                )\n",
        "            else:\n",
        "                final_model = MultiOutputRegressor(\n",
        "                    cuml_rf(\n",
        "                        n_estimators=best_params[\"n_estimators\"],\n",
        "                        max_depth=best_params[\"max_depth\"]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        '''\n",
        "        Treina o modelo final com os dados de treino completos.\n",
        "        '''\n",
        "        final_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "        '''\n",
        "        Salva o modelo final juntamente com os nomes das features.\n",
        "        '''\n",
        "        joblib.dump({'model': final_model, 'feature_names': feature_names}, final_model_file)\n",
        "        print(f\"Best model for {file_path} is {best_model_name} with RMSE: {best_rmse}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cross_validate_and_save for file {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "'''\n",
        "Processa os arquivos de acordo com o tipo de previsão (`lat_long` ou `datahora`).\n",
        "'''\n",
        "def process_files(prediction_type):\n",
        "    '''\n",
        "    Define o sufixo do arquivo e as colunas alvo com base no tipo de previsão.\n",
        "    '''\n",
        "    if prediction_type == 'lat_long':\n",
        "        file_ending = 'datahora.csv'\n",
        "        target_cols = ['latitude', 'longitude']\n",
        "    else:\n",
        "        file_ending = 'lat_long.csv'\n",
        "        target_cols = ['datahora']\n",
        "    \n",
        "    files = [f for f in os.listdir(base_path) if f.startswith(\"preprocessed_train_data\") and f.endswith(file_ending)]\n",
        "    '''\n",
        "    Percorre cada arquivo e chama a função `cross_validate_and_save` para processar e validar o arquivo.\n",
        "    '''\n",
        "    for file_name in tqdm(files, desc=f\"Processing files for {prediction_type}\"):\n",
        "        file_path = os.path.join(base_path, file_name)\n",
        "        try:\n",
        "            print(f\"Processing file: {file_name}\")\n",
        "            cross_validate_and_save(file_path, target_cols, prediction_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_name}: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_files('lat_long')\n",
        "    process_files('datahora')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Realizando as previsões</h1>\n",
        "<p>\n",
        "<li>Todos os dados usados foram coletados exclusivamente do último post em relação à tarefa 3 no Moodle</li>\n",
        "<li>Os dados foram treinados exclusivamente usando os dados criados a partir dos arquivos .JSON cujo nome começa com 2024</li>\n",
        "<li>Para fazer a avaliação do modelo durante a validação cruzada, os datasets de treino foram divididos entre treino e test, como normalmente é feito</li>\n",
        "<li>Os modelos criados durante a fase de treinamento foram utilizados nos datasets de teste disponibilizados no último link.</li>\n",
        "<li>Como previamente os datasets de treino haviam sido divididos para aplicar os modelos de previsão de datahora e para aplicar os modelos de previsão de latitude e longitude, colunas específicas foram adicionadas ou retiradas aos dois datasets de teste</li>\n",
        "<li>Para o dataset de teste onde o modelo de prever a datahora foi aplicado, as colunas temporais foram retiradas para evitar o data leakage</li>\n",
        "<li>Para o dataset de teste onde o modelo de prever latitude e longitude foi aplicado, as colunas com informações geoespaciais foram retiradas para evitar o data leakage.</li>\n",
        "<li>Após a previsão foram criados dois arquivos: um contendo todas as colunas anteriores, mais as previsões de datahora chamado predicted_datahora e outro contendo todas as colunas anteriores mais as previsões de latitude e longitude chamado predicted_lat_long</li>\n",
        " </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Logging\n",
        "'''\n",
        "logging.basicConfig(level=logging.INFO, filename='prediction.log', filemode='w',\n",
        "                    format='%(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "'''\n",
        "Define os caminhos para os diretórios base, modelo final e respostas.\n",
        "'''\n",
        "base_path = 'dados_finais/feature_modeling'\n",
        "final_model_path = 'dados_finais/cross_validation/final_model'\n",
        "respostas_path = 'dados_finais/respostas'\n",
        "\n",
        "'''\n",
        "Garante que os diretórios para salvar as respostas existem, criando-os se necessário.\n",
        "'''\n",
        "os.makedirs(respostas_path, exist_ok=True)\n",
        "\n",
        "'''\n",
        "Realiza a previsão dos valores utilizando um modelo treinado. \n",
        "Recebe como parâmetros o arquivo de teste, o arquivo do modelo, as colunas alvo e o arquivo de saída \n",
        "para salvar as previsões.\n",
        "'''\n",
        "def predict_values(test_file, model_file, target_cols, output_file):\n",
        "    logging.info(f\"Processing file {test_file} with model {model_file}\")\n",
        "    \n",
        "    '''\n",
        "    Lê o arquivo CSV de teste.\n",
        "    '''\n",
        "    df = cudf.read_csv(test_file)\n",
        "    logging.info(f\"Initial shape of the dataframe: {df.shape}\")\n",
        "\n",
        "    '''\n",
        "    Salva e remove as colunas necessárias.\n",
        "    '''\n",
        "    date_hour_from_file_test = df['date_hour_from_file_test'].unique()[0]\n",
        "    id_column = df['id']\n",
        "    \n",
        "    '''\n",
        "    Lista de colunas a serem removidas.\n",
        "    '''\n",
        "    drop_columns = ['hour_from_file_test', 'date_hour_from_file_test', 'date_from_file', 'datahora_converted', 'id']\n",
        "    \n",
        "    '''\n",
        "    Verifica se as colunas existem antes de removê-las.\n",
        "    '''\n",
        "    existing_columns = [col for col in drop_columns if col in df.columns]\n",
        "    df = df.drop(columns=existing_columns)\n",
        "    \n",
        "    logging.info(f\"Shape of the dataframe after dropping columns: {df.shape}\")\n",
        "\n",
        "    '''\n",
        "    Carrega o modelo e os nomes das features.\n",
        "    '''\n",
        "    model_data = joblib.load(model_file)\n",
        "    model = model_data['model']\n",
        "    feature_names = model_data['feature_names']\n",
        "\n",
        "    '''\n",
        "    Verifica se há features faltantes no conjunto de teste.\n",
        "    '''\n",
        "    missing_features = set(feature_names) - set(df.columns)\n",
        "    if missing_features:\n",
        "        logging.error(f\"Missing features in test data: {missing_features}\")\n",
        "        raise ValueError(f\"Missing features in test data: {missing_features}\")\n",
        "\n",
        "    '''\n",
        "    Reordena as colunas para corresponder aos dados de treino.\n",
        "    '''\n",
        "    df = df[feature_names]\n",
        "    \n",
        "    '''\n",
        "    Converte para pandas para compatibilidade com a previsão do modelo.\n",
        "    '''\n",
        "    df_pd = df.to_pandas()\n",
        "\n",
        "    '''\n",
        "    Realiza as previsões.\n",
        "    '''\n",
        "    predictions = model.predict(df_pd)\n",
        "    logging.info(f\"Predictions made. Shape of predictions: {predictions.shape}\")\n",
        "\n",
        "    '''\n",
        "    Garante que as previsões tenham a forma correta.\n",
        "    '''\n",
        "    if isinstance(predictions, pd.Series):\n",
        "        predictions = predictions.to_numpy()\n",
        "\n",
        "    if predictions.ndim == 1:\n",
        "        predictions = predictions.reshape(-1, 1)\n",
        "    \n",
        "    '''\n",
        "    Adiciona as previsões ao DataFrame.\n",
        "    '''\n",
        "    for i, col in enumerate(target_cols):\n",
        "        if i < predictions.shape[1]:\n",
        "            df[col] = predictions[:, i]\n",
        "        else:\n",
        "            logging.error(f\"Predictions do not have enough columns for target {col}\")\n",
        "            raise ValueError(f\"Predictions do not have enough columns for target {col}\")\n",
        "\n",
        "    '''\n",
        "    Insere novamente a coluna `id` no DataFrame.\n",
        "    '''\n",
        "    df.insert(0, 'id', id_column)\n",
        "    logging.info(f\"Shape of the dataframe after adding predictions: {df.shape}\")\n",
        "\n",
        "    '''\n",
        "    Salva o arquivo final de previsões.\n",
        "    '''\n",
        "    df.to_csv(output_file, index=False)\n",
        "    logging.info(f\"Predicted values saved to {output_file}\")\n",
        "\n",
        "'''\n",
        "Processa os arquivos de teste para previsões de `datahora`. \n",
        "Ela procura por modelos correspondentes e realiza previsões, salvando os resultados.\n",
        "Modelos correspondentes são aqueles cujo nome possuem a mesma data extraída do arquivo zip e a mesma\n",
        "hora extraída do arquivo json.\n",
        "'''\n",
        "def process_datahora_predictions():\n",
        "    test_files = [f for f in os.listdir(base_path) if f.startswith(\"preprocessed_treated_test_lat_long\") and f.endswith(\".csv\")]\n",
        "    model_files = [f for f in os.listdir(final_model_path) if f.startswith(\"final_model_predict_datahora\") and f.endswith(\".pkl\")]\n",
        "\n",
        "    '''\n",
        "    Percorre cada arquivo de teste e realiza previsões usando o modelo correspondente.\n",
        "    '''\n",
        "    for test_file in tqdm(test_files, desc=\"Processing datahora predictions\"):\n",
        "        date, hour_from_file_test = test_file.split('_')[-2:]\n",
        "        hour_from_file_test = hour_from_file_test.split('.')[0]\n",
        "        date_hour_from_file_test = f\"{date}_{hour_from_file_test}\"\n",
        "        output_file = os.path.join(respostas_path, f\"resposta_predicted_datahora_{date_hour_from_file_test}.csv\")\n",
        "\n",
        "        '''\n",
        "        Verifica se o arquivo de previsão já existe para evitar processamento repetido.\n",
        "        '''\n",
        "        if os.path.exists(output_file):\n",
        "            logging.info(f\"Prediction file {output_file} already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        matching_model = None\n",
        "        '''\n",
        "        Procura pelo modelo correspondente ao arquivo de teste.\n",
        "        '''\n",
        "        for model_file in model_files:\n",
        "            model_date, hour1, hour2 = model_file.split('_')[-3:]\n",
        "            hour2 = hour2.split('.')[0]\n",
        "            if model_date == date and int(hour_from_file_test) == int(hour2) + 1:\n",
        "                matching_model = os.path.join(final_model_path, model_file)\n",
        "                break\n",
        "\n",
        "        '''\n",
        "        Se encontrar um modelo correspondente, realiza a previsão; caso contrário, registra um erro.\n",
        "        '''\n",
        "        if matching_model:\n",
        "            try:\n",
        "                predict_values(os.path.join(base_path, test_file), matching_model, ['datahora'], output_file)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {test_file} with model {matching_model}: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            logging.error(f\"No matching model found for test file {test_file}\")\n",
        "            raise ValueError(f\"No matching model found for test file {test_file}\")\n",
        "\n",
        "'''\n",
        "Processa os arquivos de teste para previsões de `latitude` e `longitude`. \n",
        "Ela procura por modelos correspondentes e realiza previsões, salvando os resultados.\n",
        "'''\n",
        "def process_lat_long_predictions():\n",
        "    test_files = [f for f in os.listdir(base_path) if f.startswith(\"preprocessed_treated_test_datahora\") and f.endswith(\".csv\")]\n",
        "    model_files = [f for f in os.listdir(final_model_path) if f.startswith(\"final_model_predict_lat_long\") and f.endswith(\".pkl\")]\n",
        "\n",
        "    '''\n",
        "    Loop que percorre cada arquivo de teste e realiza previsões usando o modelo correspondente.\n",
        "    '''\n",
        "    for test_file in tqdm(test_files, desc=\"Processing lat_long predictions\"):\n",
        "        date, hour_from_file_test = test_file.split('_')[-2:]\n",
        "        hour_from_file_test = hour_from_file_test.split('.')[0]\n",
        "        date_hour_from_file_test = f\"{date}_{hour_from_file_test}\"\n",
        "        output_file = os.path.join(respostas_path, f\"resposta_predicted_lat_long_{date_hour_from_file_test}.csv\")\n",
        "\n",
        "        '''\n",
        "        Verifica se o arquivo de previsão já existe para evitar processamento repetido.\n",
        "        '''\n",
        "        if os.path.exists(output_file):\n",
        "            logging.info(f\"Prediction file {output_file} already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        matching_model = None\n",
        "        '''\n",
        "        Procura pelo modelo correspondente ao arquivo de teste.\n",
        "        '''\n",
        "        for model_file in model_files:\n",
        "            model_date, hour1, hour2 = model_file.split('_')[-3:]\n",
        "            hour2 = hour2.split('.')[0]\n",
        "            if model_date == date and int(hour_from_file_test) == int(hour2) + 1:\n",
        "                matching_model = os.path.join(final_model_path, model_file)\n",
        "                break\n",
        "\n",
        "        '''\n",
        "        Se encontrar um modelo correspondente, realiza a previsão; caso contrário, registra um erro.\n",
        "        '''\n",
        "        if matching_model:\n",
        "            try:\n",
        "                predict_values(os.path.join(base_path, test_file), matching_model, ['latitude', 'longitude'], output_file)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {test_file} with model {matching_model}: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            logging.error(f\"No matching model found for test file {test_file}\")\n",
        "            raise ValueError(f\"No matching model found for test file {test_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_lat_long_predictions()\n",
        "    process_datahora_predictions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Criando os arquivos de submissão</h1>\n",
        "<p>Como especificado no enunciado, o formato dos arquivos de submissão deveriam ser em .json. Então eu fiz um arquivo .json para cada previsão feita. <b>O grande problema com esse approach é que não será possível submeter essas respostas, haja vista que isso gerou arquivos .json muito grande. </b> </p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Para a previsão de datahora\n",
        "'''\n",
        "{ \n",
        "   \"aluno\": \"Your Name\",\n",
        "   \"datahora\": \"2024-05-20 13:00:00\" #Mesmo valor do nome do arquivo .json,\n",
        "   \"previsoes\": [ \n",
        "                  [400172783234, 1716205511000] #Id e a previsão de datahora previstos, \n",
        "                  [282474448123, 1716204264000]\n",
        "                ], \n",
        "   \"senha\": \"your_password\"\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Para a previsão de latitude e longitude\n",
        "'''\n",
        "{ \n",
        "   \"aluno\": \"Your Name\",\n",
        "   \"datahora\": \"2024-05-20 13:00:00\" #Mesmo valor do nome do arquivo .json,\n",
        "   \"previsoes\": [ \n",
        "                  [362511850614, -22.82553, -43.16925] #Id e valores de latitude e longitude previstos, \n",
        "                  [288961216441, -23.0202, -43.46159]\n",
        "                ], \n",
        "   \"senha\": \"your_password\"\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Define os caminhos para os diretórios base e de submissão, garantindo que os diretórios existam.\n",
        "'''\n",
        "root_path = os.getcwd()\n",
        "base_path = os.path.join(root_path, 'dados_finais', 'respostas')\n",
        "submission_path = os.path.join(base_path, 'submission')\n",
        "\n",
        "os.makedirs(submission_path, exist_ok=True)\n",
        "\n",
        "'''\n",
        "Função `create_json_from_csv` lê um arquivo CSV, extrai as informações necessárias e cria um arquivo JSON com o formato especificado.\n",
        "Recebe como parâmetros o arquivo CSV, o nome do aluno e a senha.\n",
        "'''\n",
        "def create_json_from_csv(csv_file, aluno, senha):\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    '''\n",
        "    Extrai a data e hora do nome do arquivo.\n",
        "    '''\n",
        "    file_name = os.path.basename(csv_file)\n",
        "    date_hour_part = file_name.split('_')[-2] + '_' + file_name.split('_')[-1].split('.')[0]\n",
        "    datahora_datetime = pd.to_datetime(date_hour_part, format='%Y-%m-%d_%H').strftime('%Y-%m-%d %H:00:00')\n",
        "\n",
        "    '''\n",
        "    Determina o tipo de previsão com base no nome do arquivo.\n",
        "    '''\n",
        "    if 'resposta_predicted_datahora_' in csv_file:\n",
        "        prediction_type = 'datahora'\n",
        "        json_file = os.path.join(submission_path, os.path.basename(csv_file).replace('resposta_predicted_', '').replace('.csv', '.json'))\n",
        "        previsoes = df[['id', 'datahora']].values.tolist()\n",
        "\n",
        "    elif 'resposta_predicted_lat_long_' in csv_file:\n",
        "        prediction_type = 'lat_long'\n",
        "        json_file = os.path.join(submission_path, os.path.basename(csv_file).replace('resposta_predicted_', '').replace('.csv', '.json'))\n",
        "        previsoes = df[['id', 'latitude', 'longitude']].values.tolist()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prediction type for file {csv_file}\")\n",
        "\n",
        "    '''\n",
        "    Cria a estrutura do JSON.\n",
        "    '''\n",
        "    # Create the JSON structure\n",
        "    json_data = {\n",
        "        \"aluno\": aluno,\n",
        "        \"datahora\": datahora_datetime,\n",
        "        \"previsoes\": previsoes,\n",
        "        \"senha\": senha\n",
        "    }\n",
        "\n",
        "    '''\n",
        "    Salva o arquivo JSON.\n",
        "    '''\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Created {json_file}\")\n",
        "\n",
        "'''\n",
        "Percorre todos os arquivos CSV na pasta base e chama a função `create_json_from_csv` para cada um.\n",
        "Recebe como parâmetros o nome do aluno e a senha.\n",
        "'''\n",
        "def process_csv_files(aluno, senha):\n",
        "    csv_files = [f for f in os.listdir(base_path) if f.startswith(\"resposta_predicted_\") and f.endswith(\".csv\")]\n",
        "    \n",
        "    '''\n",
        "    Percorre cada arquivo CSV e chama a função `create_json_from_csv`.\n",
        "    '''\n",
        "    for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
        "        csv_file_path = os.path.join(base_path, csv_file)\n",
        "        create_json_from_csv(csv_file_path, aluno, senha)\n",
        "''' \n",
        "Executa tudo\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    aluno = \"Wagner Luiz Lobo Ferreira\"\n",
        "    senha = \"muramassa_coi\"\n",
        "    \n",
        "    process_csv_files(aluno, senha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Sending the answers via API</h1>\n",
        "<p>Para enviar os arquivos .json contendo as previsões, eu usei uma API disponibilizada pelo professor, dentro de um loop.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Define o diretório contendo os arquivos JSON que serão enviados.\n",
        "'''\n",
        "base_dir = os.getcwd()\n",
        "resposta_dir = os.path.join(base_dir, 'dados_finais', 'respostas', 'submission')\n",
        "\n",
        "'''\n",
        "Define o endpoint da API para onde os arquivos JSON serão enviados.\n",
        "'''\n",
        "api_url = 'https://barra.cos.ufrj.br:443/rest/rpc/avalia'\n",
        "\n",
        "'''\n",
        "Define as credenciais do aluno que serão usadas na autenticação dos envios.\n",
        "'''\n",
        "aluno = \"Wagner Luiz Lobo Ferreira\"\n",
        "senha = \"muramassa_coi\"\n",
        "\n",
        "'''\n",
        "Função `send_json_file` envia um arquivo JSON para a API. \n",
        "Recebe como parâmetro o caminho do arquivo JSON.\n",
        "'''\n",
        "def send_json_file(file_path):\n",
        "    '''\n",
        "    Abre e carrega o conteúdo do arquivo JSON.\n",
        "    '''\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        json_data = json.load(json_file)\n",
        "    \n",
        "    '''\n",
        "    Adiciona as credenciais do aluno aos dados JSON.\n",
        "    '''\n",
        "    json_data['aluno'] = aluno\n",
        "    json_data['senha'] = senha\n",
        "    \n",
        "    '''\n",
        "    Loop que tenta enviar o arquivo JSON para a API até obter uma resposta de sucesso ou falha definitiva.\n",
        "    '''\n",
        "    while True:\n",
        "        try:\n",
        "            '''\n",
        "            Envia uma requisição POST para a API com os dados JSON.\n",
        "            '''\n",
        "            response = requests.post(\n",
        "                api_url,\n",
        "                headers={\n",
        "                    'accept': 'application/json',\n",
        "                    'Content-Type': 'application/json'\n",
        "                },\n",
        "                data=json.dumps(json_data),\n",
        "                timeout=10 \n",
        "            )\n",
        "            '''\n",
        "            Verifica se a resposta da API foi bem-sucedida (código 200).\n",
        "            '''\n",
        "            if response.status_code == 200:\n",
        "                print(f\"File {os.path.basename(file_path)} sent successfully.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Failed to send file {os.path.basename(file_path)}. Status code: {response.status_code}, Response: {response.text}\")\n",
        "                break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            '''\n",
        "            Em caso de erro, aguarda 5 segundos antes de tentar novamente.\n",
        "            '''\n",
        "            print(f\"Error sending file {os.path.basename(file_path)}: {e}. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "'''\n",
        "Itera sobre todos os arquivos JSON no diretório e envia cada um deles.\n",
        "'''\n",
        "for file_name in os.listdir(resposta_dir):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(resposta_dir, file_name)\n",
        "        send_json_file(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
